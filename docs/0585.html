<html>
<head>
<title>Node.js web scraping tutorial - LogRocket Blog</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Node.js网页抓取教程- LogRocket博客</h1>
<blockquote>原文：<a href="https://blog.logrocket.com/node-js-web-scraping-tutorial/#0001-01-01">https://blog.logrocket.com/node-js-web-scraping-tutorial/#0001-01-01</a></blockquote><div><article class="article-post">
<p><strong> <em>编者的</em> </strong> <strong> <em>注</em> </strong> : <em>本Node.js网页抓取教程最后更新于2022年1月25日；更新了所有过时的信息，并添加了关于节点爬网程序包的新部分。</em></p>
<p>在这个Node.js web抓取教程中，我们将演示如何在Node.js中构建一个web爬虫来抓取网站并将检索到的数据存储在Firebase数据库中。我们的网络爬虫将使用Node.js工作线程执行网络抓取和数据传输。</p>
<p>以下是我们将要介绍的内容:</p>

<h2 id="what-is-web-crawler">什么是网络爬虫？</h2>
<p>网络爬虫，通常简称为crawler或称为spiderbot，是一种系统地浏览互联网的机器人，通常用于<a href="https://en.wikipedia.org/wiki/Web_indexing">网络索引</a>。这些互联网机器人可以被搜索引擎用来提高用户搜索结果的质量。</p>
<h2 id="what-web-scraping-node-js">Node.js中的网页抓取是什么？</h2>
<p>除了索引万维网，爬行还可以收集数据。这就是所谓的网页抓取。</p>
<p>网络抓取的用例包括从零售商网站或旅游网站的酒店列表中收集价格，从电子邮件目录中抓取销售线索，以及收集信息来训练机器学习模型。</p>
<p>根据网站的结构和提取的数据的复杂性，网页抓取的过程对CPU来说是相当繁重的。您可以使用工作线程来优化在Node.js中执行web抓取所需的CPU密集型操作。</p>
<h2>Node.js网页抓取的安装</h2>
<p>启动终端并为本教程创建一个新目录:</p>
<pre class="language-javascript hljs">$ mkdir worker-tutorial
$ cd worker-tutorial
</pre>
<p>通过运行以下命令初始化目录:</p>
<pre class="language-javascript hljs">$ yarn init -y
</pre>
<p>我们还需要以下包来构建爬虫:</p>

<p>如果你不熟悉如何建立一个Firebase数据库，查看一下<a href="https://firebase.google.com/docs/web/setup">文档</a>，按照步骤1到3开始。</p>
<p>现在，让我们用下面的命令安装上面列出的软件包:</p>
<pre class="language-javascript hljs">$ yarn add axios cheerio firebase-admin
</pre>
<h2 id="what-is-worker-node-js">Node.js中的工作者是什么？</h2>
<p>在我们开始使用workers构建爬虫之前，让我们回顾一些基础知识。您可以在项目的根目录下创建一个测试文件<code>hello.js</code>，来运行下面的代码片段。</p>
<h3>在Node.js中注册一个工人</h3>
<p>可以通过从<code>worker_threads</code>模块导入<code>worker</code>类来初始化(注册)一个worker，如下所示:</p>
<pre class="language-javascript hljs">// hello.js

const { Worker } = require('worker_threads');

new Worker("./worker.js");
</pre>
<h3>用Node.js中的workers打印<code>Hello World</code></h3>
<p>用workers打印出<code>Hello World</code>就像运行下面的代码片段一样简单:</p>
<pre class="language-javascript hljs">// hello.js

const { Worker, isMainThread }  = require('worker_threads');
if(isMainThread){
    new Worker(__filename);
} else{
    console.log("Worker says: Hello World"); // prints 'Worker says: Hello World'
}
</pre>
<p>这个代码片段从<code>worker_threads</code>模块中提取worker类和<code>isMainThread</code>对象:</p>
<ul>
<li>帮助我们知道何时在主线程或工作线程中运行</li>
<li><code>new Worker(__filename)</code>用<code>__filename</code>变量注册一个新的工人，在本例中，变量是<code>hello.js</code></li>
</ul>
<h3>与Node.js中的工作线程通信</h3>
<p>当一个新的工作线程产生时，有一个允许线程间通信的消息传递端口。下面的代码片段显示了如何在工作线程之间传递消息:</p>
<pre class="language-javascript hljs">// hello.js

const { Worker, isMainThread, parentPort }  = require('worker_threads');

if (isMainThread) {
    const worker =  new Worker(__filename);
    worker.once('message', (message) =&gt; {
        console.log(message); // prints 'Worker thread: Hello!'
    });
    worker.postMessage('Main Thread: Hi!');
} else {
    parentPort.once('message', (message) =&gt; {
        console.log(message) // prints 'Main Thread: Hi!'
        parentPort.postMessage("Worker thread: Hello!");
    });
}
</pre>
<p>在上面的代码片段中，我们在初始化一个工作线程后使用<code>parentPort.postMessage()</code>向父线程发送一条消息。然后，我们使用<code>parentPort.once()</code>监听来自父线程的消息。</p>
<p>我们还使用<code>worker.postMessage()</code>向工作线程发送消息，并使用<code>worker.once()</code>监听来自工作线程的消息。</p>
<p>运行代码会产生以下输出:</p>
<pre class="language-javascript hljs">Main Thread: Hi!
Worker thread: Hello!
</pre>
<h2 id="how-create-web-crawler-node-js">如何用Node.js创建网络爬虫？</h2>
<p>让我们构建一个基本的web爬行器，它使用节点工作器来爬行和写入数据库。爬网程序将按以下顺序完成其任务:</p>
<ol>
<li>从<a href="https://www.iban.com/exchange-rates">网站</a>获取(请求)HTML</li>
<li>从响应中提取HTML</li>
<li>遍历DOM并提取包含汇率的表</li>
<li>格式化表格元素(<code>tbody</code>、<code>tr</code>和<code>td</code>)并提取汇率值</li>
<li>将汇率值存储在一个对象中，并使用<code>worker.postMessage()</code>将其发送给一个工作线程</li>
<li>使用<code>parentPort.on()</code>在工作线程中接受来自父线程的消息</li>
<li>在Firestore (Firebase数据库)中存储消息</li>
</ol>
<p>让我们在项目目录中创建两个新文件:</p>
<ul>
<li><code>main.js</code>为主线程</li>
<li><code>dbWorker.js</code>为工作线程</li>
</ul>
<p>本教程的源代码可以在GitHub 上找到。你可以随意复制它，叉它，或者提交一个问题。</p>
<h2 id="How-scrape-website-node-js">我怎么用Node.js刮一个网站？</h2>
<p>在主线程(<code>main.js</code>)中，我们将从<a href="https://www.iban.com/exchange-rates"> IBAN网站</a>中获取当前流行货币对美元的汇率。然后我们将导入<code>axios</code>,并用它通过一个简单的<code>GET</code>请求从站点获取HTML。</p>
<p>我们还将使用<code>cheerio</code>来遍历DOM并从table元素中提取数据。为了知道要提取的确切元素，我们将在浏览器中打开<a href="https://www.iban.com/exchange-rates"> IBAN网站</a>，并加载<a href="https://www.google.com/search?q=devtools&amp;oq=devtools&amp;aqs=chrome..69i57j0l4j69i60l2j69i61.2226j0j7&amp;sourceid=chrome&amp;ie=UTF-8">开发工具</a>:</p>
<p><img data-attachment-id="91955" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/loading-devtools/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png" data-orig-size="730,456" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Loading devtools" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools-300x187.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png" decoding="async" class="aligncenter wp-image-91955 size-full jetpack-lazy-image" src="../Images/7b6560c6386793cfce9c935e01ea42e4.png" alt="Loading Devtools In IBAN" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools-300x187.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="91955" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/loading-devtools/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png" data-orig-size="730,456" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Loading devtools" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools-300x187.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png" decoding="async" loading="lazy" class="aligncenter wp-image-91955 size-full" src="../Images/7b6560c6386793cfce9c935e01ea42e4.png" alt="Loading Devtools In IBAN" srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools-300x187.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/loading-devtools.png"/></noscript>
<p>从上图中，我们可以看到带有类的<code>table</code>元素:</p>
<pre class="language-javascript hljs">table table-bordered table-hover downloads. 
</pre>
<p>这将是一个很好的起点，我们可以将它输入到我们的<code>cheerio</code>根元素选择器中:</p>
<pre class="language-javascript hljs">// main.js

const axios = require('axios');
const cheerio = require('cheerio');
const url = "https://www.iban.com/exchange-rates";

fetchData(url).then( (res) =&gt; {
    const html = res.data;
    const $ = cheerio.load(html);
    const statsTable = $('.table.table-bordered.table-hover.downloads &gt; tbody &gt; tr');
    statsTable.each(function() {
        let title = $(this).find('td').text();
        console.log(title);
    });
})

async function fetchData(url){
    console.log("Crawling data...")
    // make http call to url
    let response = await axios(url).catch((err) =&gt; console.log(err));

    if(response.status !== 200){
        console.log("Error occurred while fetching data");
        return;
    }
    return response;
}
</pre>
<p>使用Node运行上面的代码将得到以下输出:</p>
<p><img data-attachment-id="91957" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/crawling-data-example/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png" data-orig-size="730,456" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Crawling data example" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example-300x187.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png" decoding="async" class="aligncenter size-full wp-image-91957 jetpack-lazy-image" src="../Images/da4e316dc919e90d55aa7a315f549966.png" alt="Crawling The Data From The Example In Terminal" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example-300x187.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="91957" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/crawling-data-example/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png" data-orig-size="730,456" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Crawling data example" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example-300x187.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-91957" src="../Images/da4e316dc919e90d55aa7a315f549966.png" alt="Crawling The Data From The Example In Terminal" srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example-300x187.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/crawling-data-example.png"/></noscript>
<p>接下来，我们将更新<code>main.js</code>文件，这样我们就可以正确格式化我们的输出，并将其发送到我们的工作线程。</p>
<h3>更新主线程</h3>
<p>为了正确格式化我们的输出，我们必须去掉空白和制表符，因为我们将把最终输出存储在<code>JSON</code>中。让我们相应地更新<code>main.js</code>文件:</p>
<pre class="language-javascript hljs">// main.js
[...]
let workDir = __dirname+"/dbWorker.js";

const mainFunc = async () =&gt; {
  const url = "https://www.iban.com/exchange-rates";
  // fetch html data from iban website
  let res = await fetchData(url);
  if(!res.data){
    console.log("Invalid data Obj");
    return;
  }
  const html = res.data;
  let dataObj = new Object();
  // mount html page to the root element
  const $ = cheerio.load(html);

  let dataObj = new Object();
  const statsTable = $('.table.table-bordered.table-hover.downloads &gt; tbody &gt; tr');
  //loop through all table rows and get table data
  statsTable.each(function() {
    let title = $(this).find('td').text(); // get the text in all the td elements
    let newStr = title.split("\t"); // convert text (string) into an array
    newStr.shift(); // strip off empty array element at index 0
    formatStr(newStr, dataObj); // format array string and store in an object
  });

  return dataObj;
}

mainFunc().then((res) =&gt; {
    // start worker
    const worker = new Worker(workDir); 
    console.log("Sending crawled data to dbWorker...");
    // send formatted data to worker thread 
    worker.postMessage(res);
    // listen to message from worker thread
    worker.on("message", (message) =&gt; {
        console.log(message)
    });
});

[...]

function formatStr(arr, dataObj){
    // regex to match all the words before the first digit
    let regExp = /[^A-Z]*(^\D+)/ 
    let newArr = arr[0].split(regExp); // split array element 0 using the regExp rule
    dataObj[newArr[1]] = newArr[2]; // store object 
}
</pre>
<p>在上面的代码片段中，我们做的不仅仅是数据格式化；在<code>mainFunc()</code>解析之后，我们将格式化的数据传递给<code>worker</code>线程进行存储。</p>
<h2 id="using-worker-threads-web-scraping-node-js">在Node.js中使用工作线程进行web抓取</h2>
<p>在这个工作线程中，我们将初始化Firebase，并监听来自主线程的抓取数据。当数据到达时，我们将把它存储在数据库中，并向主线程发回一条消息，确认数据存储成功。</p>
<p>负责上述操作的代码片段如下所示:</p>
<pre class="language-javascript hljs">// dbWorker.js

const { parentPort } = require('worker_threads');
const admin = require("firebase-admin");

//firebase credentials
let firebaseConfig = {
    apiKey: "XXXXXXXXXXXX-XXX-XXX",
    authDomain: "XXXXXXXXXXXX-XXX-XXX",
    databaseURL: "XXXXXXXXXXXX-XXX-XXX",
    projectId: "XXXXXXXXXXXX-XXX-XXX",
    storageBucket: "XXXXXXXXXXXX-XXX-XXX",
    messagingSenderId: "XXXXXXXXXXXX-XXX-XXX",
    appId: "XXXXXXXXXXXX-XXX-XXX"
};

// Initialize Firebase
admin.initializeApp(firebaseConfig);
let db = admin.firestore();
// get current data in DD-MM-YYYY format
let date = new Date();
let currDate = `${date.getDate()}-${date.getMonth()}-${date.getFullYear()}`;
// recieve crawled data from main thread
parentPort.once("message", (message) =&gt; {
    console.log("Recieved data from mainWorker...");
    // store data gotten from main thread in database
    db.collection("Rates").doc(currDate).set({
        rates: JSON.stringify(message)
    }).then(() =&gt; {
        // send data back to main thread if operation was successful
        parentPort.postMessage("Data saved successfully");
    })
    .catch((err) =&gt; console.log(err))    
});
</pre>
<p>用Node运行<code>main.js</code>(包含<code>dbWorker.js</code>)将得到以下输出:</p>
<p><img data-attachment-id="91959" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/node-output/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png" data-orig-size="730,201" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Node output" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output-300x83.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png" decoding="async" class="aligncenter size-full wp-image-91959 jetpack-lazy-image" src="../Images/c77d1dae3c6c4825fec22c9c6d5d4ba2.png" alt="Node Output In Terminal" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/node-output-300x83.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="91959" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/node-output/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png" data-orig-size="730,201" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Node output" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output-300x83.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-91959" src="../Images/c77d1dae3c6c4825fec22c9c6d5d4ba2.png" alt="Node Output In Terminal" srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/node-output-300x83.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/node-output.png"/></noscript>
<p>现在，您可以检查Firebase数据库，并查看以下已爬网数据:</p>
<p><img data-attachment-id="91961" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/firebase-database-2/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png" data-orig-size="730,418" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Firebase database" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database-300x172.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png" decoding="async" class="aligncenter size-full wp-image-91961 jetpack-lazy-image" src="../Images/d69faa547a3469d92b3faf2906b35882.png" alt="Loading Firebase Data" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database-300x172.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="91961" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/firebase-database-2/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png" data-orig-size="730,418" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Firebase database" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database-300x172.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-91961" src="../Images/d69faa547a3469d92b3faf2906b35882.png" alt="Loading Firebase Data" srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database-300x172.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/Firebase-database.png"/></noscript>
<h2 id="crawling-pages-node-crawler">使用节点爬虫抓取页面</h2>
<p>我们上面实现的方法利用了两个不同的包(Axios和Cheerios)来获取和遍历网页。</p>
<p>只使用<a href="https://github.com/bda-research/node-crawler">节点爬虫</a>，我们可以轻松地执行这些功能。node-crawler在引擎盖下使用Cheerio，并带有额外的功能，允许您定制您抓取和抓取网站的方式。</p>
<p>您可以指定选项，如一次可以执行的最大请求数(<code>maxConnections</code>)、请求之间允许的最短时间(<code>rateLimit</code>)、请求失败时允许的重试次数以及每个请求的优先级。</p>
<p>显然，节点爬虫有很多优点。让我们看看它的代码是如何工作的。</p>
<h3>正在安装节点爬网程序</h3>
<p>在项目目录中，运行以下命令:</p>
<pre class="language-javascript hljs">npm install crawler
</pre>
<p>在名为<code>crawler.js</code>的文件中，添加以下代码:</p>
<pre class="language-javascript hljs">const Crawler = require('crawler');
const crawlerInstance = new Crawler({
    maxConnections: 10,

    callback: (error, res, done) =&gt; {
        if (error) {
            console.log(error);
        } else {
            const $ = res.$;
            const statsTable = 
            $('.table.table-bordered.table-hover.downloads &gt; tbody &gt; tr');
            statsTable.each(function() {
                let title = $(this).find('td').text();
                console.log(title);
            });
        }
        done();
    }
});

crawlerInstance.queue('https://www.iban.com/exchange-rates');
</pre>
<p>这里，我们使用一个包——节点爬虫——来获取一个网页并遍历它的DOM。我们将它的包导入到我们的项目中，并创建一个名为<code>crawlerInstance</code>的实例。</p>
<p><code>maxConnection</code>选项指定一次要执行的任务数量。在这种情况下，我们将其设置为<code>10</code>。接下来，我们创建一个回调函数，它在获取网页后执行。行<code>const $ = res.$</code>使Cheerio在刚刚获取的网页中可用。</p>
<h3>使用节点爬虫获取数据</h3>
<p>接下来，类似于我们之前所做的，我们遍历IBAN汇率页面，获取表上的数据，并在控制台中显示它们。</p>
<p><code>queue</code>函数负责获取网页的数据，在前面的例子中，这个任务是由Axios执行的。</p>
<p><img data-attachment-id="91963" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/node-crawler-working/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png" data-orig-size="730,276" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="node-crawler working" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working-300x113.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png" decoding="async" class="aligncenter size-full wp-image-91963 jetpack-lazy-image" src="../Images/7d87aa77358e28c63b8309d7177b6e31.png" alt="Node-Crawler Working" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working-300x113.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="91963" data-permalink="https://blog.logrocket.com/node-js-web-scraping-tutorial/node-crawler-working/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png" data-orig-size="730,276" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="node-crawler working" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working-300x113.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-91963" src="../Images/7d87aa77358e28c63b8309d7177b6e31.png" alt="Node-Crawler Working" srcset="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working-300x113.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/02/node-crawler-working.png"/></noscript>
<p>要一次从多个网页获取数据，将所有URL添加到<code>queue</code>,如下所示:</p>
<pre class="language-javascript hljs">crawlerInstance.queue(['https://www.iban.com/exchange-rates','http://www.facebook.com']);
</pre>
<p>默认情况下，node-crawler使用实例化时创建的回调函数(全局回调)。要为特定任务创建自定义回调函数，只需将其添加到<code>queue</code>请求中:</p>
<pre class="language-javascript hljs">crawlerInstance.queue([{
    uri: 'http://www.facebook.com',

    callback: (error, res, done) =&gt; {
        if (error) {
            console.log(error);
        } else {
            console.log('res.body.length');
        }
        done();
    }
}]);
</pre>
<h3>使用节点爬虫添加瓶颈</h3>
<p>如上所述，使用节点爬虫的一个优点是它允许你定制你的网络抓取任务，并给它们添加瓶颈。</p>
<p>现在，您可能想知道为什么需要有目的地给任务添加瓶颈。嗯，网站往往有反抓取机制，可以检测和阻止你的请求，如果他们都同时执行。</p>
<p>使用节点爬虫的<code>rateLimit</code>，可以在请求之间添加时间间隔，以确保它们不会同时执行。</p>
<p>如前所述，<code>maxConnection</code>还可以通过限制可以同时执行的查询数量来增加任务的瓶颈。以下是如何使用这两个选项:</p>
<pre class="language-javascript hljs">const crawlerInstance = new Crawler({
    rateLimit: 2000,
    maxConnections: 1,
    callback: (error, res, done) =&gt; {
        if (error) {
            console.log(error);
        } else {
            const $ = res.$;
            console.log($('body').text());
        }
        done();
    }
});
</pre>
<p>当<code>rateLimit</code>设置为<code>2000</code>时，请求之间会有2秒的间隔。</p>
<h2 id="legal-use-web-scraper">使用刮网器合法吗？</h2>
<p>虽然网络抓取很有趣，但如果你使用数据侵犯版权，它也可能是违法的。一般来说，建议您事先阅读您打算爬取的网站的条款和条件，以了解他们的数据爬取政策。</p>
<p>在开始你自己的Node.js网页抓取项目之前，你可以<a href="https://en.m.wikipedia.org/wiki/Web_crawler">了解更多关于网页抓取策略的信息</a>。</p>
<p>使用工作线程并不能保证您的应用程序会更快，但如果有效使用，可以呈现这种幻影，因为它通过减少主线程上CPU密集型任务的麻烦来释放主线程。</p>
<h2>结论</h2>
<p>在本教程中，我们学习了如何构建一个抓取货币汇率并将其保存到数据库中的网络爬虫。我们还学习了如何使用工作线程来运行这些操作。</p>
<p>以下每个片段的源代码都可以在<a href="https://github.com/Jordanirabor/workers-tutorial"> GitHub </a>上找到。你可以随意复制它，叉它，或者提交一个问题。</p><div class="code-block code-block-23">
<div class="blog-plug inline-plug node-plug"><h2>200只<img src="../Images/61167b9d027ca73ed5aaf59a9ec31267.png" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2019/10/green-check.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://blog.logrocket.com/wp-content/uploads/2019/10/green-check.png"/> <noscript> <img data-lazy-fallback="1" src="../Images/61167b9d027ca73ed5aaf59a9ec31267.png" data-original-src="https://blog.logrocket.com/wp-content/uploads/2019/10/green-check.png"/> </noscript>显示器出现故障，生产中网络请求缓慢</h2><p>部署基于节点的web应用程序或网站是容易的部分。确保您的节点实例继续为您的应用程序提供资源是事情变得更加困难的地方。如果您对确保对后端或第三方服务的请求成功感兴趣，</p><a href="https://lp.logrocket.com/blg/node-signup" target="_blank">try LogRocket</a><p>. </p><a class="signup" href="https://lp.logrocket.com/blg/node-signup" target="_blank" rel="noopener noreferrer"><img src="../Images/cae72fd2a54c5f02a6398c4867894844.png" alt="LogRocket Network Request Monitoring" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2019/12/network-request-filter-2-1.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" class=" jetpack-lazy-image" data-original-src="https://blog.logrocket.com/wp-content/uploads/2019/12/network-request-filter-2-1.png"/><noscript><img data-lazy-fallback="1" src="../Images/cae72fd2a54c5f02a6398c4867894844.png" alt="LogRocket Network Request Monitoring" data-original-src="https://blog.logrocket.com/wp-content/uploads/2019/12/network-request-filter-2-1.png"/></noscript></a><a href="https://lp.logrocket.com/blg/node-signup" target="_blank" rel="noopener noreferrer">https://logrocket.com/signup/</a><p>LogRocket 就像是网络和移动应用程序的DVR，记录下用户与你的应用程序交互时发生的一切。您可以汇总并报告有问题的网络请求，以快速了解根本原因，而不是猜测问题发生的原因。</p><p>LogRocket检测您的应用程序以记录基线性能计时，如页面加载时间、到达第一个字节的时间、慢速网络请求，还记录Redux、NgRx和Vuex操作/状态。</p><a class="signup" href="https://lp.logrocket.com/blg/node-signup" target="_blank" rel="noopener noreferrer">Start monitoring for free</a><p>. </p></div>
</div>

<p class="clearfix"/>
<p class="clearfix"/>
</article>

</div>    
</body>
</html>