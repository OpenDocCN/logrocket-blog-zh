<html>
<head>
<title>Using Whisper for speech recognition in React Native </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>在React Native中使用Whisper进行语音识别</h1>
<blockquote>原文：<a href="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/#0001-01-01">https://blog.logrocket.com/using-whisper-speech-recognition-react-native/#0001-01-01</a></blockquote><div><article class="article-post">
<p>在本文中，我们将使用Whisper创建一个语音到文本的应用程序。Whisper需要Python后端，所以我们将使用Flask为应用程序创建服务器。</p>
<p>React Native作为构建我们的移动客户端的框架。我希望你喜欢创建这个应用程序的过程，因为我确实喜欢。让我们开始吧。</p>
<p><em>向前跳转:</em></p>

<h2 id="speech-recognition">什么是语音识别？</h2>
<p>语音识别使程序能够将人的语音处理成书面格式。语法、句法、结构和音频对于理解和处理人类语言是必不可少的。</p>
<p>语音识别算法是计算机科学中最复杂的领域之一。人工智能、<a href="https://blog.logrocket.com/machine-learning-in-rust-using-linfa/">机器学习</a>，无监督预训练技术的发展，以及在自我监督学习和从原始音频学习方面有效的<a href="https://arxiv.org/abs/2006.11477"> Wav2Vec 2.0 </a>等框架，都提升了他们的能力。</p>
<p>语音识别器由以下组件组成:</p>
<ul>
<li>语音输入</li>
<li>解码器，依赖声学模型、发音词典和语言模型进行输出</li>
<li>单词输出</li>
</ul>
<p>这些组件和技术进步使得大量未标记语音数据集的消费成为可能。预训练的音频编码器能够学习高质量的语音表示；它们唯一的缺点是不受监督。</p>
<h3 id="decoder">什么是解码器？</h3>
<p>性能解码器将语音表示映射到可用的输出。解码器解决了音频编码器的监控问题。但是，解码器限制了Wav2Vec等框架对语音识别的有效性。解码器使用起来可能非常复杂，需要熟练的从业者，尤其是因为Wav2Vec 2.0这样的技术很难使用。</p>
<p>关键是结合尽可能多的高质量语音识别数据集。以这种方式训练的模型比基于单一来源训练的模型更有效。</p>
<h2 id="what-is-whisper">什么是耳语？</h2>
<p>Whisper，或<a href="https://openai.com/blog/whisper/"> WSPR </a>，代表网络规模的语音识别监督预训练。耳语模型接受训练，能够预测文本的文字。</p>
<p>Whisper依靠<a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">序列到序列模型</a>来映射话语及其转录形式，这使得语音识别管道更加有效。Whisper配备了一个音频语言检测器，这是一个经过<a href="http://bark.phon.ioc.ee/voxlingua107/">vox linga 107</a>训练的微调模型。</p>
<p>Whisper数据集由音频和来自互联网的文字记录组成。数据集的质量随着自动过滤方法的使用而提高。</p>
<h2 id="setting-whisper">设置耳语</h2>
<p>要使用Whisper，我们需要依赖Python作为我们的后端。Whisper还需要命令行工具<a href="https://ffmpeg.org/"> ffmpeg </a>，它使我们的应用程序能够记录、转换和流式传输音频和视频。</p>
<p>下面是在不同的机器上安装ffgmeg的必要命令:</p>
<pre class="language-bash hljs"># on Ubuntu or Debian
sudo apt update &amp;&amp; sudo apt install ffmpeg


# on Arch Linux
sudo pacman -S ffmpeg


# on MacOS using Homebrew (https://brew.sh/)
brew install ffmpeg


# on Windows using Chocolatey (https://chocolatey.org/)
choco install ffmpeg


# on Windows using Scoop (https://scoop.sh/)
scoop install ffmpeg
</pre>
<h2 id="creating-backend-application">用Flask创建后端应用程序</h2>
<p>在本节中，我们将为我们的应用程序创建后端服务。<a href="https://flask.palletsprojects.com/en/2.2.x/installation/"> Flask </a>是用Python写的web框架。我选择使用Flask作为这个应用程序，因为它易于设置。</p>
<p>Flask开发团队<a href="https://flask.palletsprojects.com/en/2.2.x/installation/">推荐使用最新版本的Python </a>，尽管Flask仍然支持Python ≥ 3.7。</p>
<p>一旦先决条件的安装完成，我们就可以创建项目文件夹来保存我们的客户端和后端应用程序。</p>
<pre class="language-bash hljs">mkdir translateWithWhisper &amp;&amp; cd translateWithWhisper &amp;&amp; mkdir backend &amp;&amp; cd backend
</pre>
<p>Flask利用虚拟环境来管理项目依赖性；Python有一个现成的<a href="https://docs.python.org/3/library/venv.html#module-venv"> venv模块</a>来创建它们。</p>
<p>在终端窗口中使用下面的命令创建<code>venv</code>文件夹。这个文件夹包含我们的依赖项。</p>
<pre class="language-bash hljs">python3 -m venv venv</pre>
<h3 id="specifying-project-dependencies">指定项目依赖关系</h3>
<p>使用一个<code>requirements.txt</code>文件，指定必要的依赖关系。<code>requirements.txt</code>文件位于后端目录的根目录下。</p>
<pre class="language-plaintext hljs">touch requirements.txt
code requirements.txt</pre>
<p>将以下代码复制并粘贴到<code>requirements.txt</code>文件中:</p>
<pre class="language-bash hljs">numpy
tqdm
transformers&gt;=4.19.0
ffmpeg-python==0.2.0
pyaudio
SpeechRecognition
pydub
git+https://github.com/openai/whisper.git
--extra-index-url https://download.pytorch.org/whl/cu113
torch
flask
flask_cors
</pre>
<h3 id="creating-bash-shell-script-install-dependencies">创建Bash shell脚本来安装依赖项</h3>
<p>在根项目目录中，创建一个Bash shell脚本文件。Bash脚本处理Flask应用程序中依赖项的安装。</p>
<p>在根项目目录中，打开终端窗口。使用以下命令创建shell脚本:</p>
<pre class="language-bash hljs">touch install_dependencies.sh
code install_dependencies.sh</pre>
<p>将以下代码块复制并粘贴到<code>install_dependencies.sh</code>文件中:</p>
<pre class="language-bash hljs"># install and run backend
cd backend &amp;&amp; python3 -m venv venv
source venv/Scripts/activate

pip install wheel
pip install -r requirements.txt
</pre>
<p>现在，在根目录中打开一个终端窗口，并运行以下命令:</p>
<pre class="language-shell hljs">sh .\install_dependencies.sh</pre>
<h2 id="creating-transcribe-endpoint">创建一个<code>transcribe</code>端点</h2>
<p>现在，我们将在应用程序中创建一个<code>transcribe</code>端点，它将从客户端接收音频输入。应用程序将转录输入并将转录的文本返回给客户端。</p>
<p>这个端点接受一个<code>POST</code>请求并处理输入。当响应是200 HTTP响应时，客户机接收转录的文本。</p>
<p>创建一个<code>app.py</code>文件来保存处理输入的逻辑。打开一个新的终端窗口，并在后端目录中创建一个<code>app.py</code>文件:</p>
<pre class="language-bash hljs">touch backend/app.py
code backend/app.py</pre>
<p>将下面的代码块复制并粘贴到<code>app.py</code>文件中:</p>
<pre class="language-python hljs">import os
import tempfile
import flask
from flask import request
from flask_cors import CORS
import whisper

app = flask.Flask(__name__)
CORS(app)

// endpoint for handling the transcribing of audio inputs
@app.route('/transcribe', methods=['POST'])
def transcribe():
    if request.method == 'POST
        language = request.form['language']
        model = request.form['model_size']

        # there are no english models for large
        if model != 'large' and language == 'english':
            model = model + '.en'
        audio_model = whisper.load_model(model)

        temp_dir = tempfile.mkdtemp()
        save_path = os.path.join(temp_dir, 'temp.wav')

        wav_file = request.files['audio_data']
        wav_file.save(save_path)

        if language == 'english':
            result = audio_model.transcribe(save_path, language='english')
        else:
            result = audio_model.transcribe(save_path)

        return result['text']
    else:
        return "This endpoint only processes POST wav blob"
</pre>
<h3 id="run-flask-application">运行烧瓶应用程序</h3>
<p>在包含<code>venv</code>变量的激活终端窗口中，运行以下命令启动应用程序:</p>
<pre class="language-bash hljs">$ cd backend
$ flask run –port 8000</pre>
<p>期望应用程序启动时没有任何错误。如果是这种情况，在终端窗口中应该可以看到以下结果:</p>
<p><img data-attachment-id="144897" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/error-terminal/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png" data-orig-size="730,119" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="error-terminal" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal-300x49.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png" decoding="async" class="aligncenter wp-image-144897 size-full jetpack-lazy-image" src="../Images/11c3d98e3ccbc1c9b562c07d27bdd249.png" alt="Error in the terminal" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png 730w, https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal-300x49.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="144897" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/error-terminal/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png" data-orig-size="730,119" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="error-terminal" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal-300x49.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png" decoding="async" loading="lazy" class="aligncenter wp-image-144897 size-full" src="../Images/11c3d98e3ccbc1c9b562c07d27bdd249.png" alt="Error in the terminal" srcset="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png 730w, https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal-300x49.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/error-terminal.png"/></noscript>
<p>这就结束了Flask应用程序中的<code>transcribe</code>端点的创建。</p>
<h2 id="hosting-server">托管服务器</h2>
<p>要向iOS中创建的HTTP端点发出网络请求，我们需要路由到HTTPS服务器。ngrok 解决了创建重路由的问题。</p>
<p><a href="https://ngrok.com/download">下载ngrok </a>，然后安装包并打开。一个终端窗口启动；输入以下命令以使用ngrok托管服务器:</p>
<pre class="language-bash hljs">ngrok http 8000</pre>
<p>ngrok将生成一个托管的URL，该URL将在客户端应用程序中用于请求。</p>
<p><img data-attachment-id="144900" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/hosted-url/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png" data-orig-size="730,428" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hosted-url" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url-300x176.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png" decoding="async" class="aligncenter wp-image-144900 size-full jetpack-lazy-image" src="../Images/225d56dd591ca49721e9cb52eb9014d7.png" alt="Hosted URL" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png 730w, https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url-300x176.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="144900" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/hosted-url/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png" data-orig-size="730,428" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hosted-url" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url-300x176.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png" decoding="async" loading="lazy" class="aligncenter wp-image-144900 size-full" src="../Images/225d56dd591ca49721e9cb52eb9014d7.png" alt="Hosted URL" srcset="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png 730w, https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url-300x176.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/hosted-url.png"/></noscript>
<h2 id="creating-speech-recognition-mobile-application-react-native">使用React Native创建语音识别移动应用程序</h2>
<p>对于教程的这一部分，您需要安装一些东西:</p>
<ul>
<li>Expo CLI :与Expo工具接口的命令行工具</li>
<li>用于Android和iOS的Expo Go应用程序:用于打开通过Expo CLI提供的应用程序</li>
</ul>
<p>在新的终端窗口中，初始化React本地项目:</p>
<pre class="language-bash hljs">npx create-expo-app client
cd client</pre>
<p>现在，启动开发服务器:</p>
<pre class="language-bash hljs">npx expo start</pre>
<p>要在iOS设备上打开应用程序，请打开相机并扫描终端上的二维码。在Android设备上，按下Expo Go应用程序的<strong>主页</strong>标签上的<strong>扫描二维码</strong>。</p>
<figure id="attachment_144902" aria-describedby="caption-attachment-144902" class="wp-caption aligncenter"><img data-attachment-id="144902" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/expo-go-app/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app.png" data-orig-size="730,1579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="expo-go-app" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-139x300.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-473x1024.png" decoding="async" class="wp-image-144902 size-full jetpack-lazy-image" src="../Images/c6044bd3b1eb8e3a927b9a75c2fd067b.png" alt="Expo go app" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app.png 730w, https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-139x300.png 139w, https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-473x1024.png 473w, https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-710x1536.png 710w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app.png"/><noscript><img data-lazy-fallback="1" data-attachment-id="144902" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/expo-go-app/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app.png" data-orig-size="730,1579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="expo-go-app" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-139x300.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-473x1024.png" decoding="async" loading="lazy" class="wp-image-144902 size-full" src="../Images/c6044bd3b1eb8e3a927b9a75c2fd067b.png" alt="Expo go app" srcset="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app.png 730w, https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-139x300.png 139w, https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-473x1024.png 473w, https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app-710x1536.png 710w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/expo-go-app.png"/></noscript><figcaption id="caption-attachment-144902" class="wp-caption-text">Our Expo Go app</figcaption></figure>
<h3 id="handling-audio-recording">处理录音</h3>
<p><a href="https://docs.expo.dev/versions/latest/sdk/av/"> Expo-av </a>处理我们应用程序中的音频录制。我们的Flask服务器希望文件采用<code>.wav</code>格式。expo-av包允许我们在保存前指定格式。</p>
<p>在终端中安装必要的软件包:</p>
<pre class="language-bash hljs">yarn add axios expo-av react-native-picker-select</pre>
<h3 id="creating-model-selector">创建模型选择器</h3>
<p>应用程序必须能够选择模型大小。有五个选项可供选择:</p>
<ul>
<li>微小的</li>
<li>基础</li>
<li>小的</li>
<li>中等</li>
<li>大的</li>
</ul>
<p>选定的输入大小决定了在服务器上将输入与什么模型进行比较。</p>
<p>再次在终端中，使用以下命令创建一个<code>src</code>文件夹和一个名为<code>/components</code>的子文件夹:</p>
<pre class="language-bash hljs">mkdir src
mkdir src/components
touch src/components/Mode.tsx
code src/components/Mode.tsx</pre>
<p>将代码块粘贴到<code>Mode.tsx</code>文件中:</p>
<pre class="language-typescript hljs">import React from "react";
import { View, Text, StyleSheet } from "react-native";
import RNPickerSelect from "react-native-picker-select";

const Mode = ({
  onModelChange,
  transcribeTimeout,
  onTranscribeTimeoutChanged,
}: any) =&gt; {
  function onModelChangeLocal(value: any) {
    onModelChange(value);
  }

  function onTranscribeTimeoutChangedLocal(event: any) {
    onTranscribeTimeoutChanged(event.target.value);
  }

  return (
    &lt;View&gt;
      &lt;Text style={styles.title}&gt;Model Size&lt;/Text&gt;
      &lt;View style={{ flexDirection: "row" }}&gt;
        &lt;RNPickerSelect
          onValueChange={(value) =&gt; onModelChangeLocal(value)}
          useNativeAndroidPickerStyle={false}
          placeholder={{ label: "Select model", value: null }}
          items={[
            { label: "tiny", value: "tiny" },
            { label: "base", value: "base" },
            { label: "small", value: "small" },
            { label: "medium", value: "medium" },
            { label: "large", value: "large" },
          ]}
          style={customPickerStyles}
        /&gt;
      &lt;/View&gt;
      &lt;View&gt;
        &lt;Text style={styles.title}&gt;Timeout :{transcribeTimeout}&lt;/Text&gt;
      &lt;/View&gt;
    &lt;/View&gt;
  );
};

export default Mode;
const styles = StyleSheet.create({
  title: {
    fontWeight: "200",
    fontSize: 25,
    float: "left",
  },
});
const customPickerStyles = StyleSheet.create({
  inputIOS: {
    fontSize: 14,
    paddingVertical: 10,
    paddingHorizontal: 12,
    borderWidth: 1,
    borderColor: "green",
    borderRadius: 8,
    color: "black",
    paddingRight: 30, // to ensure the text is never behind the icon
  },
  inputAndroid: {
    fontSize: 14,
    paddingHorizontal: 10,
    paddingVertical: 8,
    borderWidth: 1,
    borderColor: "blue",
    borderRadius: 8,
    color: "black",
    paddingRight: 30, // to ensure the text is never behind the icon
  },
});
</pre>
<h3 id="creating-transcribe-output">创建<code>Transcribe</code>输出</h3>
<p>服务器返回带有文本的输出。该组件接收输出数据并显示它。</p>
<pre class="language-bash hljs">mkdir src
mkdir src/components
touch src/components/TranscribeOutput.tsx
code src/components/TranscribeOutput.tsx</pre>
<p>将代码块粘贴到<code>TranscribeOutput.tsx</code>文件中:</p>
<pre class="language-typescript hljs">import React from "react";
import { Text, View, StyleSheet } from "react-native";
const TranscribedOutput = ({
  transcribedText,
  interimTranscribedText,
}: any) =&gt; {
  if (transcribedText.length === 0 &amp;&amp; interimTranscribedText.length === 0) {
    return &lt;Text&gt;...&lt;/Text&gt;;
  }

  return (
    &lt;View style={styles.box}&gt;
      &lt;Text style={styles.text}&gt;{transcribedText}&lt;/Text&gt;
      &lt;Text&gt;{interimTranscribedText}&lt;/Text&gt;
    &lt;/View&gt;
  );
};
const styles = StyleSheet.create({
  box: {
    borderColor: "black",
    borderRadius: 10,
    marginBottom: 0,
  },
  text: {
    fontWeight: "400",
    fontSize: 30,
  },
});

export default TranscribedOutput;
</pre>
<h3 id="creating-client-functionality">创建客户端功能</h3>
<p>应用程序依靠<a href="https://www.npmjs.com/package/axios"> Axios </a>来发送和接收来自Flask服务器的数据；我们在前面的部分安装了它。测试应用程序的默认语言是英语。</p>
<p>在<code>App.tsx</code>文件中，导入必要的依赖关系:</p>
<pre class="language-typescript hljs">import * as React from "react";
import {
  Text,
  StyleSheet,
  View,
  Button,
  ActivityIndicator,
} from "react-native";
import { Audio } from "expo-av";
import FormData from "form-data";
import axios from "axios";
import Mode from "./src/components/Mode";
import TranscribedOutput from "./src/components/TranscribeOutput";
</pre>
<h3 id="creating-state-variables">创建状态变量</h3>
<p>该应用程序需要跟踪正在进行的记录、转录数据、记录和转录。默认情况下，在状态中设置语言、型号和超时。</p>
<pre class="language-typescript hljs">export default () =&gt; {
  const [recording, setRecording] = React.useState(false as any);
  const [recordings, setRecordings] = React.useState([]);
  const [message, setMessage] = React.useState("");
  const [transcribedData, setTranscribedData] = React.useState([] as any);
  const [interimTranscribedData] = React.useState("");
  const [isRecording, setIsRecording] = React.useState(false);
  const [isTranscribing, setIsTranscribing] = React.useState(false);
  const [selectedLanguage, setSelectedLanguage] = React.useState("english");
  const [selectedModel, setSelectedModel] = React.useState(1);
  const [transcribeTimeout, setTranscribeTimout] = React.useState(5);
  const [stopTranscriptionSession, setStopTranscriptionSession] =
    React.useState(false);
  const [isLoading, setLoading] = React.useState(false);
  return (
    &lt;View style={styles.root}&gt;&lt;/View&gt;
)
}

const styles = StyleSheet.create({
  root: {
    display: "flex",
    flex: 1,
    alignItems: "center",
    textAlign: "center",
    flexDirection: "column",
  },
});
</pre>
<h3 id="creating-references-languages-model-options-variables">创建引用、语言和模型选项变量</h3>
<p><a href="https://reactjs.org/docs/hooks-reference.html#useref"> useRef钩子</a>使我们能够跟踪当前初始化的属性。我们想在转录会话、语言和模型上设置<code>useRef</code>。</p>
<p>将代码块粘贴到<code>setLoading</code> <code>useState</code>钩子下:</p>
<pre class="language-typescript hljs">  const [isLoading, setLoading] = React.useState(false);
  const intervalRef: any = React.useRef(null);

  const stopTranscriptionSessionRef = React.useRef(stopTranscriptionSession);
  stopTranscriptionSessionRef.current = stopTranscriptionSession;

  const selectedLangRef = React.useRef(selectedLanguage);
  selectedLangRef.current = selectedLanguage;

  const selectedModelRef = React.useRef(selectedModel);
  selectedModelRef.current = selectedModel;

  const supportedLanguages = [
    "english",
    "chinese",
    "german",
    "spanish",
    "russian",
    "korean",
    "french",
    "japanese",
    "portuguese",
    "turkish",
    "polish",
    "catalan",
    "dutch",
    "arabic",
    "swedish",
    "italian",
    "indonesian",
    "hindi",
    "finnish",
    "vietnamese",
    "hebrew",
    "ukrainian",
    "greek",
    "malay",
    "czech",
    "romanian",
    "danish",
    "hungarian",
    "tamil",
    "norwegian",
    "thai",
    "urdu",
    "croatian",
    "bulgarian",
    "lithuanian",
    "latin",
    "maori",
    "malayalam",
    "welsh",
    "slovak",
    "telugu",
    "persian",
    "latvian",
    "bengali",
    "serbian",
    "azerbaijani",
    "slovenian",
    "kannada",
    "estonian",
    "macedonian",
    "breton",
    "basque",
    "icelandic",
    "armenian",
    "nepali",
    "mongolian",
    "bosnian",
    "kazakh",
    "albanian",
    "swahili",
    "galician",
    "marathi",
    "punjabi",
    "sinhala",
    "khmer",
    "shona",
    "yoruba",
    "somali",
    "afrikaans",
    "occitan",
    "georgian",
    "belarusian",
    "tajik",
    "sindhi",
    "gujarati",
    "amharic",
    "yiddish",
    "lao",
    "uzbek",
    "faroese",
    "haitian creole",
    "pashto",
    "turkmen",
    "nynorsk",
    "maltese",
    "sanskrit",
    "luxembourgish",
    "myanmar",
    "tibetan",
    "tagalog",
    "malagasy",
    "assamese",
    "tatar",
    "hawaiian",
    "lingala",
    "hausa",
    "bashkir",
    "javanese",
    "sundanese",
  ];

  const modelOptions = ["tiny", "base", "small", "medium", "large"];
  React.useEffect(() =&gt; {
    return () =&gt; clearInterval(intervalRef.current);
  }, []);

  function handleTranscribeTimeoutChange(newTimeout: any) {
    setTranscribeTimout(newTimeout);
  }
</pre>
<h2 id="creating-recording-functions">创建录制功能</h2>
<p>在这一节中，我们将编写五个函数来处理音频转录。</p>
<h3 id="startrecording-function"><code>startRecording</code>功能</h3>
<p>第一个函数是<code>startRecording</code>函数。该功能使应用程序能够请求使用麦克风的许可。所需的音频格式是预设的，我们有一个ref用于跟踪超时:</p>
<pre class="language-typescript hljs">  async function startRecording() {
    try {
      console.log("Requesting permissions..");
      const permission = await Audio.requestPermissionsAsync();
      if (permission.status === "granted") {
        await Audio.setAudioModeAsync({
          allowsRecordingIOS: true,
          playsInSilentModeIOS: true,
        });
        alert("Starting recording..");
        const RECORDING_OPTIONS_PRESET_HIGH_QUALITY: any = {
          android: {
            extension: ".mp4",
            outputFormat: Audio.RECORDING_OPTION_ANDROID_OUTPUT_FORMAT_MPEG_4,
            audioEncoder: Audio.RECORDING_OPTION_ANDROID_AUDIO_ENCODER_AMR_NB,
            sampleRate: 44100,
            numberOfChannels: 2,
            bitRate: 128000,
          },
          ios: {
            extension: ".wav",
            audioQuality: Audio.RECORDING_OPTION_IOS_AUDIO_QUALITY_MIN,
            sampleRate: 44100,
            numberOfChannels: 2,
            bitRate: 128000,
            linearPCMBitDepth: 16,
            linearPCMIsBigEndian: false,
            linearPCMIsFloat: false,
          },
        };
        const { recording }: any = await Audio.Recording.createAsync(
          RECORDING_OPTIONS_PRESET_HIGH_QUALITY
        );
        setRecording(recording);
        console.log("Recording started");
        setStopTranscriptionSession(false);
        setIsRecording(true);
        intervalRef.current = setInterval(
          transcribeInterim,
          transcribeTimeout * 1000
        );
        console.log("erer", recording);
      } else {
        setMessage("Please grant permission to app to access microphone");
      }
    } catch (err) {
      console.error(" Failed to start recording", err);
    }
  }
</pre>
<h3 id="stoprecording-function"><code>stopRecording</code>功能</h3>
<p><code>stopRecording</code>功能使用户能够停止录制。<code>recording</code>状态变量存储并保存更新的记录。</p>
<pre class="language-typescript hljs">  async function stopRecording() {
    console.log("Stopping recording..");
    setRecording(undefined);
    await recording.stopAndUnloadAsync();
    const uri = recording.getURI();
    let updatedRecordings = [...recordings] as any;
    const { sound, status } = await recording.createNewLoadedSoundAsync();
    updatedRecordings.push({
      sound: sound,
      duration: getDurationFormatted(status.durationMillis),
      file: recording.getURI(),
    });
    setRecordings(updatedRecordings);
    console.log("Recording stopped and stored at", uri);
    // Fetch audio binary blob data

    clearInterval(intervalRef.current);
    setStopTranscriptionSession(true);
    setIsRecording(false);
    setIsTranscribing(false);
  }</pre>
<h3 id="getdurationformatted-getrecordinglines-functions"><code>getDurationFormatted</code>和<code>getRecordingLines</code>功能</h3>
<p>要获得录制的持续时间和录制文本的长度，创建<code>getDurationFormatted</code>和<code>getRecordingLines</code>函数:</p>
<pre class="language-typescript hljs">  function getDurationFormatted(millis: any) {
    const minutes = millis / 1000 / 60;
    const minutesDisplay = Math.floor(minutes);
    const seconds = Math.round(minutes - minutesDisplay) * 60;
    const secondDisplay = seconds &lt; 10 ? `0${seconds}` : seconds;
    return `${minutesDisplay}:${secondDisplay}`;
  }

  function getRecordingLines() {
    return recordings.map((recordingLine: any, index) =&gt; {
      return (
        &lt;View key={index} style={styles.row}&gt;
          &lt;Text style={styles.fill}&gt;
            {" "}
            Recording {index + 1} - {recordingLine.duration}
          &lt;/Text&gt;
          &lt;Button
            style={styles.button}
            onPress={() =&gt; recordingLine.sound.replayAsync()}
            title="Play"
          &gt;&lt;/Button&gt;
        &lt;/View&gt;
      );
    });
  }
</pre>
<h3 id="create-transcriberecording-function">创建<code>transcribeRecording</code>功能</h3>
<p>这个函数允许我们与我们的Flask服务器通信。我们使用expo-av库中的<code>getURI()</code>功能访问我们创建的音频。<code>language</code>、<code>model_size</code>和<code>audio_data</code>是我们发送给服务器的关键数据。</p>
<p>一个<code>200</code>响应表示成功。我们将响应存储在<code>setTranscribedData</code> <code>useState</code>钩子中。此回复包含我们的转录文本。</p>
<pre class="language-typescript hljs">function transcribeInterim() {
    clearInterval(intervalRef.current);
    setIsRecording(false);
  }

  async function transcribeRecording() {
    const uri = recording.getURI();
    const filetype = uri.split(".").pop();
    const filename = uri.split("/").pop();
    setLoading(true);
    const formData: any = new FormData();
    formData.append("language", selectedLangRef.current);
    formData.append("model_size", modelOptions[selectedModelRef.current]);
    formData.append(
      "audio_data",
      {
        uri,
        type: `audio/${filetype}`,
        name: filename,
      },
      "temp_recording"
    );
    axios({
      url: "https://2c75-197-210-53-169.eu.ngrok.io/transcribe",
      method: "POST",
      data: formData,
      headers: {
        Accept: "application/json",
        "Content-Type": "multipart/form-data",
      },
    })
      .then(function (response) {
        console.log("response :", response);
        setTranscribedData((oldData: any) =&gt; [...oldData, response.data]);
        setLoading(false);
        setIsTranscribing(false);
        intervalRef.current = setInterval(
          transcribeInterim,
          transcribeTimeout * 1000
        );
      })
      .catch(function (error) {
        console.log("error : error");
      });

    if (!stopTranscriptionSessionRef.current) {
      setIsRecording(true);
    }
  }
</pre>
<h2 id="assembling-application">组装应用程序</h2>
<p>让我们组装到目前为止创建的所有部件:</p>
<pre class="language-typescript hljs">import * as React from "react";
import {
  Text,
  StyleSheet,
  View,
  Button,
  ActivityIndicator,
} from "react-native";
import { Audio } from "expo-av";
import FormData from "form-data";
import axios from "axios";
import Mode from "./src/components/Mode";
import TranscribedOutput from "./src/components/TranscribeOutput";

export default () =&gt; {
  const [recording, setRecording] = React.useState(false as any);
  const [recordings, setRecordings] = React.useState([]);
  const [message, setMessage] = React.useState("");
  const [transcribedData, setTranscribedData] = React.useState([] as any);
  const [interimTranscribedData] = React.useState("");
  const [isRecording, setIsRecording] = React.useState(false);
  const [isTranscribing, setIsTranscribing] = React.useState(false);
  const [selectedLanguage, setSelectedLanguage] = React.useState("english");
  const [selectedModel, setSelectedModel] = React.useState(1);
  const [transcribeTimeout, setTranscribeTimout] = React.useState(5);
  const [stopTranscriptionSession, setStopTranscriptionSession] =
    React.useState(false);
  const [isLoading, setLoading] = React.useState(false);
  const intervalRef: any = React.useRef(null);

  const stopTranscriptionSessionRef = React.useRef(stopTranscriptionSession);
  stopTranscriptionSessionRef.current = stopTranscriptionSession;

  const selectedLangRef = React.useRef(selectedLanguage);
  selectedLangRef.current = selectedLanguage;

  const selectedModelRef = React.useRef(selectedModel);
  selectedModelRef.current = selectedModel;

  const supportedLanguages = [
    "english",
    "chinese",
    "german",
    "spanish",
    "russian",
    "korean",
    "french",
    "japanese",
    "portuguese",
    "turkish",
    "polish",
    "catalan",
    "dutch",
    "arabic",
    "swedish",
    "italian",
    "indonesian",
    "hindi",
    "finnish",
    "vietnamese",
    "hebrew",
    "ukrainian",
    "greek",
    "malay",
    "czech",
    "romanian",
    "danish",
    "hungarian",
    "tamil",
    "norwegian",
    "thai",
    "urdu",
    "croatian",
    "bulgarian",
    "lithuanian",
    "latin",
    "maori",
    "malayalam",
    "welsh",
    "slovak",
    "telugu",
    "persian",
    "latvian",
    "bengali",
    "serbian",
    "azerbaijani",
    "slovenian",
    "kannada",
    "estonian",
    "macedonian",
    "breton",
    "basque",
    "icelandic",
    "armenian",
    "nepali",
    "mongolian",
    "bosnian",
    "kazakh",
    "albanian",
    "swahili",
    "galician",
    "marathi",
    "punjabi",
    "sinhala",
    "khmer",
    "shona",
    "yoruba",
    "somali",
    "afrikaans",
    "occitan",
    "georgian",
    "belarusian",
    "tajik",
    "sindhi",
    "gujarati",
    "amharic",
    "yiddish",
    "lao",
    "uzbek",
    "faroese",
    "haitian creole",
    "pashto",
    "turkmen",
    "nynorsk",
    "maltese",
    "sanskrit",
    "luxembourgish",
    "myanmar",
    "tibetan",
    "tagalog",
    "malagasy",
    "assamese",
    "tatar",
    "hawaiian",
    "lingala",
    "hausa",
    "bashkir",
    "javanese",
    "sundanese",
  ];

  const modelOptions = ["tiny", "base", "small", "medium", "large"];

  React.useEffect(() =&gt; {
    return () =&gt; clearInterval(intervalRef.current);
  }, []);

  function handleTranscribeTimeoutChange(newTimeout: any) {
    setTranscribeTimout(newTimeout);
  }

  async function startRecording() {
    try {
      console.log("Requesting permissions..");
      const permission = await Audio.requestPermissionsAsync();
      if (permission.status === "granted") {
        await Audio.setAudioModeAsync({
          allowsRecordingIOS: true,
          playsInSilentModeIOS: true,
        });
        alert("Starting recording..");
        const RECORDING_OPTIONS_PRESET_HIGH_QUALITY: any = {
          android: {
            extension: ".mp4",
            outputFormat: Audio.RECORDING_OPTION_ANDROID_OUTPUT_FORMAT_MPEG_4,
            audioEncoder: Audio.RECORDING_OPTION_ANDROID_AUDIO_ENCODER_AMR_NB,
            sampleRate: 44100,
            numberOfChannels: 2,
            bitRate: 128000,
          },
          ios: {
            extension: ".wav",
            audioQuality: Audio.RECORDING_OPTION_IOS_AUDIO_QUALITY_MIN,
            sampleRate: 44100,
            numberOfChannels: 2,
            bitRate: 128000,
            linearPCMBitDepth: 16,
            linearPCMIsBigEndian: false,
            linearPCMIsFloat: false,
          },
        };
        const { recording }: any = await Audio.Recording.createAsync(
          RECORDING_OPTIONS_PRESET_HIGH_QUALITY
        );
        setRecording(recording);
        console.log("Recording started");
        setStopTranscriptionSession(false);
        setIsRecording(true);
        intervalRef.current = setInterval(
          transcribeInterim,
          transcribeTimeout * 1000
        );
        console.log("erer", recording);
      } else {
        setMessage("Please grant permission to app to access microphone");
      }
    } catch (err) {
      console.error(" Failed to start recording", err);
    }
  }
  async function stopRecording() {
    console.log("Stopping recording..");
    setRecording(undefined);
    await recording.stopAndUnloadAsync();
    const uri = recording.getURI();
    let updatedRecordings = [...recordings] as any;
    const { sound, status } = await recording.createNewLoadedSoundAsync();
    updatedRecordings.push({
      sound: sound,
      duration: getDurationFormatted(status.durationMillis),
      file: recording.getURI(),
    });
    setRecordings(updatedRecordings);
    console.log("Recording stopped and stored at", uri);
    // Fetch audio binary blob data

    clearInterval(intervalRef.current);
    setStopTranscriptionSession(true);
    setIsRecording(false);
    setIsTranscribing(false);
  }

  function getDurationFormatted(millis: any) {
    const minutes = millis / 1000 / 60;
    const minutesDisplay = Math.floor(minutes);
    const seconds = Math.round(minutes - minutesDisplay) * 60;
    const secondDisplay = seconds &lt; 10 ? `0${seconds}` : seconds;
    return `${minutesDisplay}:${secondDisplay}`;
  }

  function getRecordingLines() {
    return recordings.map((recordingLine: any, index) =&gt; {
      return (
        &lt;View key={index} style={styles.row}&gt;
          &lt;Text style={styles.fill}&gt;
            {" "}
            Recording {index + 1} - {recordingLine.duration}
          &lt;/Text&gt;
          &lt;Button
            style={styles.button}
            onPress={() =&gt; recordingLine.sound.replayAsync()}
            title="Play"
          &gt;&lt;/Button&gt;
        &lt;/View&gt;
      );
    });
  }

  function transcribeInterim() {
    clearInterval(intervalRef.current);
    setIsRecording(false);
  }

  async function transcribeRecording() {
    const uri = recording.getURI();
    const filetype = uri.split(".").pop();
    const filename = uri.split("/").pop();
    setLoading(true);
    const formData: any = new FormData();
    formData.append("language", selectedLangRef.current);
    formData.append("model_size", modelOptions[selectedModelRef.current]);
    formData.append(
      "audio_data",
      {
        uri,
        type: `audio/${filetype}`,
        name: filename,
      },
      "temp_recording"
    );
    axios({
      url: "https://2c75-197-210-53-169.eu.ngrok.io/transcribe",
      method: "POST",
      data: formData,
      headers: {
        Accept: "application/json",
        "Content-Type": "multipart/form-data",
      },
    })
      .then(function (response) {
        console.log("response :", response);
        setTranscribedData((oldData: any) =&gt; [...oldData, response.data]);
        setLoading(false);
        setIsTranscribing(false);
        intervalRef.current = setInterval(
          transcribeInterim,
          transcribeTimeout * 1000
        );
      })
      .catch(function (error) {
        console.log("error : error");
      });

    if (!stopTranscriptionSessionRef.current) {
      setIsRecording(true);
    }
  }
  return (
    &lt;View style={styles.root}&gt;
      &lt;View style={{ flex: 1 }}&gt;
        &lt;Text style={styles.title}&gt;Speech to Text. &lt;/Text&gt;
        &lt;Text style={styles.title}&gt;{message}&lt;/Text&gt;
      &lt;/View&gt;
      &lt;View style={styles.settingsSection}&gt;
        &lt;Mode
          disabled={isTranscribing || isRecording}
          possibleLanguages={supportedLanguages}
          selectedLanguage={selectedLanguage}
          onLanguageChange={setSelectedLanguage}
          modelOptions={modelOptions}
          selectedModel={selectedModel}
          onModelChange={setSelectedModel}
          transcribeTimeout={transcribeTimeout}
          onTranscribeTiemoutChanged={handleTranscribeTimeoutChange}
        /&gt;
      &lt;/View&gt;
      &lt;View style={styles.buttonsSection}&gt;
        {!isRecording &amp;&amp; !isTranscribing &amp;&amp; (
          &lt;Button onPress={startRecording} title="Start recording" /&gt;
        )}
        {(isRecording || isTranscribing) &amp;&amp; (
          &lt;Button
            onPress={stopRecording}
            disabled={stopTranscriptionSessionRef.current}
            title="stop recording"
          /&gt;
        )}
        &lt;Button title="Transcribe" onPress={() =&gt; transcribeRecording()} /&gt;
        {getRecordingLines()}
      &lt;/View&gt;

      {isLoading !== false ? (
        &lt;ActivityIndicator
          size="large"
          color="#00ff00"
          hidesWhenStopped={true}
          animating={true}
        /&gt;
      ) : (
        &lt;Text&gt;&lt;/Text&gt;
      )}

      &lt;View style={styles.transcription}&gt;
        &lt;TranscribedOutput
          transcribedText={transcribedData}
          interimTranscribedText={interimTranscribedData}
        /&gt;
      &lt;/View&gt;
    &lt;/View&gt;
  );
};

const styles = StyleSheet.create({
  root: {
    display: "flex",
    flex: 1,
    alignItems: "center",
    textAlign: "center",
    flexDirection: "column",
  },
  title: {
    marginTop: 40,
    fontWeight: "400",
    fontSize: 30,
  },
  settingsSection: {
    flex: 1,
  },
  buttonsSection: {
    flex: 1,
    flexDirection: "row",
  },
  transcription: {
    flex: 1,
    flexDirection: "row",
  },
  recordIllustration: {
    width: 100,
  },
  row: {
    flexDirection: "row",
    alignItems: "center",
    justifyContent: "center",
  },
  fill: {
    flex: 1,
    margin: 16,
  },
  button: {
    margin: 16,
  },
});
</pre>
<h2 id="running-application">运行应用程序</h2>
<p>使用以下命令运行React本机应用程序:</p>
<pre class="language-bash hljs">yarn start</pre>
<p><img data-attachment-id="144905" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/running-application/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application.gif" data-orig-size="730,1582" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="running-application" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application-138x300.gif" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application-473x1024.gif" decoding="async" class="aligncenter wp-image-144905 size-full jetpack-lazy-image" src="../Images/8e395d0f6ae54e10a2e2aeddfc0e5946.png" alt="Running the application." data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application.gif?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application.gif"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="144905" data-permalink="https://blog.logrocket.com/using-whisper-speech-recognition-react-native/attachment/running-application/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application.gif" data-orig-size="730,1582" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="running-application" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application-138x300.gif" data-large-file="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application-473x1024.gif" decoding="async" loading="lazy" class="aligncenter wp-image-144905 size-full" src="../Images/8e395d0f6ae54e10a2e2aeddfc0e5946.png" alt="Running the application." data-original-src="https://blog.logrocket.com/wp-content/uploads/2022/12/running-application.gif"/></noscript>
<p><a href="https://github.com/dueka/translateWithWhisper">项目库</a>是公开可用的。</p>
<h2>结论</h2>
<p>在本文中，我们学习了如何在React本地应用程序中创建语音到文本的功能。我预见耳语会改变日常生活中叙述和听写的方式。本文涵盖的技术支持创建听写应用程序。</p>
<p>我很高兴看到新的和创新的方式，开发人员扩展了Whisper，例如，使用Whisper在我们的移动和网络设备上执行操作，或者使用Whisper改善我们的网站和应用程序的可访问性。</p><div class="code-block code-block-18">
<div class="blog-plug inline-plug react-native-plug"><h2><a href="https://lp.logrocket.com/blg/react-native-signup"> LogRocket </a>:即时重现React原生应用中的问题。</h2><a class="signup" href="https://lp.logrocket.com/blg/react-native-signup" target="_blank" rel="noopener noreferrer"><img class="alignnone size-full wp-image-46 jetpack-lazy-image" src="../Images/110055665562c1e02069b3698e6cc671.png" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/10/react-native-plug_v2-2.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/10/react-native-plug_v2-2.png"/><noscript><img data-lazy-fallback="1" class="alignnone size-full wp-image-46" src="../Images/110055665562c1e02069b3698e6cc671.png" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/10/react-native-plug_v2-2.png"/></noscript></a><p><a href="https://lp.logrocket.com/blg/react-native-signup" target="_blank" rel="noopener noreferrer"> LogRocket </a>是一款React原生监控解决方案，可帮助您即时重现问题、确定bug的优先级并了解React原生应用的性能。</p><p>LogRocket还可以向你展示用户是如何与你的应用程序互动的，从而帮助你提高转化率和产品使用率。LogRocket的产品分析功能揭示了用户不完成特定流程或不采用新功能的原因。</p><p>开始主动监控您的React原生应用— <a class="signup" href="https://lp.logrocket.com/blg/react-native-signup" target="_blank" rel="noopener noreferrer">免费试用LogRocket】。</a></p></div>
</div>

<p class="clearfix"/>
<p class="clearfix"/>
</article>

</div>    
</body>
</html>