<html>
<head>
<title>Using Elasticsearch, Logstash, and Kibana with Go applications - LogRocket Blog</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>在 Go 应用程序中使用 Elasticsearch、Logstash 和 Kibana</h1>
<blockquote>原文：<a href="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/#0001-01-01">https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/#0001-01-01</a></blockquote><div><article class="article-post">
<p>Elasticsearch 是一个开源的分布式搜索和分析引擎，基于 T2 的 Apache Lucene。与主要目的是存储数据的典型 SQL 和 NoSQL 数据库不同，Elasticsearch 存储并索引数据，以便可以快速搜索和分析。它还集成了<a href="https://www.elastic.co/logstash" target="_blank" rel="noopener"> Logstash </a>(一种可以从多个来源(如日志和数据库)获取数据的数据处理管道)和<a href="https://www.elastic.co/kibana" target="_blank" rel="noopener"> Kibana </a>(用于数据可视化)，它们一起组成了 ELK 堆栈。</p>
<p>在本教程中，我们将探讨如何结合 Elasticsearch 和 Golang 的力量。我们将建立一个基本的内容管理系统，能够创建，阅读，更新和删除帖子，以及通过 Elasticsearch 搜索帖子的能力。</p>
<h2>要求</h2>
<p>要了解本教程中的示例项目，您需要:</p>
<ul>
<li>安装在您机器上的<a href="https://golang.org/dl/" target="_blank" rel="noopener"> Go </a>(版本&gt; = 1.14)</li>
<li><a href="https://www.docker.com/" target="_blank" rel="noopener">对接器</a>和<a href="https://docs.docker.com/compose/" target="_blank" rel="noopener">对接器组合</a>安装完毕</li>
<li>熟悉 Docker 和 Go 编程语言</li>
</ul>
<h2>入门指南</h2>
<p>在您的首选位置创建一个新目录来存放项目(我将我的命名为<code>letterpress</code>)，并使用下面的命令初始化一个新的 Go 模块:</p>
<pre>$ mkdir letterpress &amp;&amp; cd letterpress
$ go mod init gitlab.com/idoko/letterpress
</pre>
<p>应用程序依赖关系包括:</p>
<ul>
<li><a href="https://github.com/lib/pq)%5Bib/pq%5D(https://github.com/lib/pq" target="_blank" rel="noopener"> lib/pq </a> —与 Go 标准库中的数据库/sql 包兼容的用于 Go 的 PostgreSQL 驱动程序</li>
<li><a href="https://pkg.go.dev/github.com/elastic/go-elasticsearch/" target="_blank" rel="noopener">elastic/go-elasticsearch</a>—Golang 官方 elastic search 客户端</li>
<li>gin-gonic/gin  —我们将为应用程序的 REST API 使用的 HTTP 框架</li>
<li>rs/zerolog  —一个轻量级的日志记录器</li>
</ul>
<p>通过在您的终端中运行以下命令来安装依赖项:<br/> <code>$ go get github.com/lib/pq github.com/elastic/go-elasticsearch github.com/gin-gonic/gin github.com/rs/zerolog</code></p>
<p>接下来，在项目目录中创建所需的文件夹和文件，以匹配下面的结构:</p>
<pre>├── cmd
│   ├── api
│   │   └── main.go
├── db
│   ├── database.go
│   └── posts.go
├── .env
├── handler
├── logstash
│   ├── config
│   ├── pipelines
│   └── queries
└── models
    └── post.go
</pre>
<ul>
<li><code>cmd</code> —这是应用程序二进制文件(即<code>main.go</code>文件)所在的位置。我们还添加了一个内部的<code>api</code>子文件夹，允许多个二进制文件，否则这是不可能的</li>
<li><code>db</code>—<code>db</code>包充当我们的应用程序和数据库之间的桥梁。我们稍后还将使用它来存储数据库迁移文件</li>
<li><code>.env</code> —包含环境变量的“键值”映射(例如，数据库凭证)</li>
<li><code>handler</code>—<code>handler</code>包包括由 gin 框架支持的 API 路由处理器</li>
<li><code>logstash</code> —这是我们保存与 logstash 相关的代码的地方，比如管道配置和附带的<code>Dockerfile</code></li>
<li><code>models</code> —模型是 Golang 结构，可以被编组到适当的 JSON 对象中</li>
</ul>
<p>打开项目根目录下的<code>.env</code>文件，设置如下环境变量:</p>
<pre>POSTGRES_USER=letterpress
POSTGRES_PASSWORD=letterpress_secrets
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=letterpress_db

ELASTICSEARCH_URL="http://elasticsearch:9200"
</pre>
<p>打开<code>post.go</code>文件(在<code>models</code>文件夹中)并设置<code>Post</code>结构:</p>
<pre>package models

type Post struct {
   ID    int    `json:"id,omitempty"`
   Title string `json:"title"`
   Body  string `json:"body"`
}
</pre>
<p>接下来，将下面的代码添加到<code>db/database.go</code>中，以管理数据库连接:</p>
<pre>package db

import (
   "database/sql"
   "fmt"
   _ "github.com/lib/pq"
   "github.com/rs/zerolog"
)

type Database struct {
   Conn *sql.DB
   Logger zerolog.Logger
}

type Config struct {
   Host     string
   Port     int
   Username string
   Password string
   DbName   string
   Logger zerolog.Logger
}

func Init(cfg Config) (Database, error) {
   db := Database{}
   dsn := fmt.Sprintf("host=%s port=%d user=%s password=%s dbname=%s sslmode=disable",
      cfg.Host, cfg.Port, cfg.Username, cfg.Password, cfg.DbName)
   conn, err := sql.Open("postgres", dsn)
   if err != nil {
      return db, err
   }

   db.Conn = conn
   db.Logger = cfg.Logger
   err = db.Conn.Ping()
   if err != nil {
      return db, err
   }
   return db, nil
}
</pre>
<p>在上面的代码中，我们设置了数据库配置并添加了一个<code>Logger</code>字段，该字段可用于记录数据库错误和事件。</p>
<p>此外，打开<code>db/posts.go</code>并为我们即将创建的<em>帖子</em>和<em>帖子日志</em>表执行数据库操作:</p>
<pre>package db

import (
   "database/sql"
   "fmt"
   "gitlab.com/idoko/letterpress/models"
)

var (
   ErrNoRecord = fmt.Errorf("no matching record found")
   insertOp = "insert"
   deleteOp = "delete"
   updateOp = "update"
)

func (db Database) SavePost(post *models.Post) error {
   var id int
   query := `INSERT INTO posts(title, body) VALUES ($1, $2) RETURNING id`
   err := db.Conn.QueryRow(query, post.Title, post.Body).Scan(&amp;id)
   if err != nil {
      return err
   }
   logQuery := `INSERT INTO post_logs(post_id, operation) VALUES ($1, $2)`
   post.ID = id
   _, err = db.Conn.Exec(logQuery, post.ID, insertOp)
   if err != nil {
      db.Logger.Err(err).Msg("could not log operation for logstash")
   }
   return nil
}
</pre>
<p>上面，我们实现了一个<code>SavePost</code>函数，它在数据库中插入了<code>Post</code>参数。如果插入成功，它会将操作和为新帖子生成的 ID 记录到一个<code>post_logs</code>表中。这些日志发生在应用程序级别，但是如果您觉得您的数据库操作不会总是通过应用程序，您可以尝试使用触发器在数据库级别完成。Logstash 稍后将使用这些日志来同步我们的 Elasticsearch 索引和我们的应用程序数据库。</p>
<p>仍然在<code>posts.go</code>文件中，添加下面的代码来更新和删除数据库中的帖子:</p>
<pre>func (db Database) UpdatePost(postId int, post models.Post) error {
   query := "UPDATE posts SET title=$1, body=$2 WHERE id=$3"
   _, err := db.Conn.Exec(query, post.Title, post.Body, postId)
   if err != nil {
      return err
   }

   post.ID = postId
   logQuery := "INSERT INTO post_logs(post_id, operation) VALUES ($1, $2)"
   _, err = db.Conn.Exec(logQuery, post.ID, updateOp)
   if err != nil {
      db.Logger.Err(err).Msg("could not log operation for logstash")
   }
   return nil
}

func (db Database) DeletePost(postId int) error {
   query := "DELETE FROM Posts WHERE id=$1"
   _, err := db.Conn.Exec(query, postId)
   if err != nil {
      if err == sql.ErrNoRows {
         return ErrNoRecord
      }
      return err
   }

   logQuery := "INSERT INTO post_logs(post_id, operation) VALUES ($1, $2)"
   _, err = db.Conn.Exec(logQuery, postId, deleteOp)
   if err != nil {
      db.Logger.Err(err).Msg("could not log operation for logstash")
   }
   return nil
}
</pre>
<h2>使用 golang-migrate 进行数据库迁移</h2>
<p>虽然 PostgreSQL 会在 Docker 容器中设置应用程序数据库时自动创建它，但是我们需要自己设置表。为此，我们将使用<a href="https://github.com/golang-migrate/migrate" target="_blank" rel="noopener"> golang-migrate/migrate </a>来管理我们的数据库<a href="https://en.wikipedia.org/wiki/Schema_migration" target="_blank" rel="noopener">迁移</a>。使用<a href="https://github.com/golang-migrate/migrate/tree/master/cmd/migrate#installation" target="_blank" rel="noopener">安装<code>migrate</code>本指南</a>并运行以下命令，为<em>帖子</em>表生成迁移文件:</p>
<pre>$ migrate create -ext sql -dir db/migrations -seq create_posts_table
$ migrate create -ext sql -dir db/migrations -seq create_post_logs_table
</pre>
<p>上面的命令将在 db/migrations 中创建四个 SQL 文件，其中两个的扩展名为<code>.up.sql</code>，而另外两个以<code>.down.sql</code>结尾。<em> Up </em>迁移是在我们应用迁移时执行的。因为我们想在我们的例子中创建表，所以将下面的代码块添加到<code>XXXXXX_create_posts_table.up.sql</code>文件中:</p>
<pre>CREATE TABLE IF NOT EXISTS posts (
    id SERIAL PRIMARY KEY,
    title VARCHAR(150),
    body text
);
</pre>
<p>类似地，打开<code>XXXXXX_create_post_logs_table.up.sql</code>并指示它创建<em> posts_logs </em>表，如下所示:</p>
<pre>CREATE TABLE IF NOT EXISTS post_logs (
    id SERIAL PRIMARY KEY,
    post_id INT NOT NULL,
    operation VARCHAR(20) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
</pre>
<p><em> Down </em>当我们想要回滚对数据库所做的更改时，就会应用迁移。在我们的例子中，我们希望删除刚刚创建的表。将下面的代码添加到<code>XXXXXX_create_posts_table.down.sql</code>中，删除<em>帖子</em>表:</p>
<pre>DROP TABLE IF EXISTS posts;
</pre>
<p>通过将下面的代码添加到<code>XXXXXX_create_post_logs_table.down.sql</code>中，对<em> posts_logs </em>表做同样的事情:</p>
<pre>DROP TABLE IF EXISTS post_logs;
</pre>
<h2>作为 Docker 容器的 Elasticsearch 和 PostgreSQL</h2>
<p>在项目根目录中创建一个<code>docker-compose.yml</code>文件，并声明我们的应用程序需要的服务，如下所示:</p>
<pre>version: "3"

services:
  postgres:
    image: postgres
    restart: unless-stopped
    hostname: postgres
    env_file: .env
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  api:
    build:
      context: .
      dockerfile: Dockerfile
    hostname: api
    env_file: .env
    ports:
      - "8080:8080"
    depends_on:
      - postgres

  elasticsearch:
    image: 'docker.elastic.co/elasticsearch/elasticsearch:7.10.2'
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data

volumes:
  pgdata:
    driver: local
  esdata:
    driver: local
</pre>
<p>这些服务包括:</p>
<ul>
<li><code>postgres</code> —我们的应用程序将使用的 PostgreSQL 数据库。它还公开了默认的 PostgreSQL 端口，以便我们可以从容器外部访问我们的数据库</li>
<li>这是我们应用程序的 REST API，允许我们创建和搜索帖子</li>
<li><code>elasticsearch</code> —为我们的搜索功能提供动力的弹性搜索图片。由于我们处于开发环境中，我们还将<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html" target="_blank" rel="noopener">发现类型</a>设置为<code>single-node</code></li>
</ul>
<p>接下来，在项目文件夹中创建项目的<code>Dockerfile</code>,并用下面的代码填充它:</p>
<pre>FROM golang:1.15.7-buster

COPY go.mod go.sum /go/src/gitlab.com/idoko/letterpress/
WORKDIR /go/src/gitlab.com/idoko/letterpress
RUN go mod download
COPY . /go/src/gitlab.com/idoko/letterpress
RUN go build -o /usr/bin/letterpress gitlab.com/idoko/letterpress/cmd/api

EXPOSE 8080 8080
ENTRYPOINT ["/usr/bin/letterpress"]
</pre>
<p>在上面的代码中，我们已经设置了 Docker 来使用 Debian buster 映像构建我们的应用程序。接下来，它下载应用程序依赖项，构建应用程序，并将生成的二进制文件复制到<code>/usr/bin</code>。</p>
<p>虽然我们还没有实现 REST API，但是您可以通过在您的终端中运行<code>docker-compose up--build</code>来启动服务，尝试一下目前的进展。</p>
<p>在 PostgreSQL 服务运行的情况下，将<em>数据源名称</em> (DSN)导出为环境变量，并应用我们通过从项目根目录运行以下命令创建的迁移:</p>
<pre>$ export PGURL="postgres://letterpress:<a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="e18d84959584939193849292be92848293849592a18d8e82808d898e9295">[email protected]</a>:5432/letterpress_db?sslmode=disable"
$ migrate -database $PGURL -path db/migrations/ up 
</pre>
<p><em>注:DSN 的格式为<code>postgres://USERNAME:<a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="1949584a4a4e564b5d5951564a4d">[email protected]</a>:PORT/DATABASE?sslmode=SSLMODE</code>。如果您的值与我们在上面的<code>.env</code>文件中使用的值不同，请记住使用您的值。</em></p>
<h2>用 gin-gonic/gin 传送处理器</h2>
<p>要设置我们的 API 路由，在<code>handlers</code>文件夹中创建一个新的<code>handler.go</code>文件，并设置它来初始化和注册相关的路由:</p>
<pre>package handler

import (
   "github.com/elastic/go-elasticsearch/v7"
   "github.com/gin-gonic/gin"
   "github.com/rs/zerolog"
   "gitlab.com/idoko/letterpress/db"
)

type Handler struct {
   DB     db.Database
   Logger zerolog.Logger
   ESClient *elasticsearch.Client
}

func New(database db.Database, esClient *elasticsearch.Client, logger zerolog.Logger) *Handler {
   return &amp;Handler{
      DB:     database,
      ESClient: esClient,
      Logger: logger,
   }
}

func (h *Handler) Register(group *gin.RouterGroup) {
   group.GET("/posts/:id", h.GetPost)
   group.PATCH("/posts/:id", h.UpdatePost)
   group.DELETE("/posts/:id", h.DeletePost)

   group.GET("/posts", h.GetPosts)
   group.POST("/posts", h.CreatePost)

   group.GET("/search", h.SearchPosts)
}
</pre>
<p>routes 向我们的帖子展示了一个 CRUD 接口，以及一个<em>搜索</em>端点，允许使用 Elasticsearch 搜索所有的帖子。</p>
<p>在同一个<code>handlers</code>目录中创建一个<code>post.go</code>文件，并添加上述路由处理程序的实现(为了简洁起见，我们将查看创建和搜索帖子，尽管您可以在<a href="https://gitlab.com/idoko/letterpress/-/blob/master/handler/post.go" target="_blank" rel="noopener">项目的 GitLab 存储库</a>中看到其他处理程序的完整实现):</p>
<pre>package handler

import (
   "context"
   "encoding/json"
   "fmt"
   "github.com/gin-gonic/gin"
   "gitlab.com/idoko/letterpress/db"
   "gitlab.com/idoko/letterpress/models"
   "net/http"
   "strconv"
   "strings"
)

func (h *Handler) CreatePost(c *gin.Context) {
   var post models.Post
   if err := c.ShouldBindJSON(&amp;post); err != nil {
      h.Logger.Err(err).Msg("could not parse request body")
      c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprintf("invalid request body: %s", err.Error())})
      return
   }
   err := h.DB.SavePost(&amp;post)
   if err != nil {
      h.Logger.Err(err).Msg("could not save post")
      c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("could not save post: %s", err.Error())})
   } else {
      c.JSON(http.StatusCreated, gin.H{"post": post})
   }
}

func (h *Handler) SearchPosts(c *gin.Context) {
   var query string
   if query, _ = c.GetQuery("q"); query == "" {
      c.JSON(http.StatusBadRequest, gin.H{"error": "no search query present"})
      return
   }

   body := fmt.Sprintf(
      `{"query": {"multi_match": {"query": "%s", "fields": ["title", "body"]}}}`,
      query)
   res, err := h.ESClient.Search(
      h.ESClient.Search.WithContext(context.Background()),
      h.ESClient.Search.WithIndex("posts"),
      h.ESClient.Search.WithBody(strings.NewReader(body)),
      h.ESClient.Search.WithPretty(),
      )
   if err != nil {
      h.Logger.Err(err).Msg("elasticsearch error")
      c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
      return
   }
   defer res.Body.Close()
   if res.IsError() {
      var e map[string]interface{}
      if err := json.NewDecoder(res.Body).Decode(&amp;e); err != nil {
         h.Logger.Err(err).Msg("error parsing the response body")
      } else {
         h.Logger.Err(fmt.Errorf("[%s] %s: %s",
            res.Status(),
            e["error"].(map[string]interface{})["type"],
            e["error"].(map[string]interface{})["reason"],
         )).Msg("failed to search query")
      }
      c.JSON(http.StatusInternalServerError, gin.H{"error": e["error"].(map[string]interface{})["reason"]})
      return
   }

   h.Logger.Info().Interface("res", res.Status())

   var r map[string]interface{}
   if err := json.NewDecoder(res.Body).Decode(&amp;r); err != nil {
      h.Logger.Err(err).Msg("elasticsearch error")
      c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
      return
   }
   c.JSON(http.StatusOK, gin.H{"data": r["hits"]})
}
</pre>
<p><code>CreatePost</code>获取 JSON 请求体，并使用 gin 的<code><a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.ShouldBindJSON" target="_blank" rel="noopener">ShouldBindJSON</a></code>将其转换成一个<code>Post</code>结构。然后使用我们之前编写的<code>SavePost</code>函数将结果对象保存到数据库中。</p>
<p><code>SearchPosts</code>比较投入。它使用 Elasticsearch 的多重查询来搜索帖子。这样，我们可以快速找到标题和/或正文包含给定查询的帖子。我们还检查并记录任何可能发生的错误，并使用 Go 标准库中的<code>json</code>包将响应转换成 JSON 对象，并将其作为用户的搜索结果呈现给用户。</p>
<h2>使用 Logstash 将数据库同步到 Elasticsearch</h2>
<p>Logstash 是一个数据处理管道，它从不同的输入源接收数据，对它们进行处理，然后将它们发送到输出源。</p>
<p>因为我们的目标是通过 Elasticsearch 搜索数据库中的数据，所以我们将配置 Logstash，使用 PostgreSQL 数据库作为输入，使用 Elasticsearch 作为输出。</p>
<p>在<code>logstash/config</code>目录中，创建一个新的<code>pipelines.yml</code>文件来保存我们将需要的所有 Logstash 管道。对于这个项目，它是一个将数据库与 Elasticsearch 同步的单一管道。在新的<code>pipelines.yml</code>中添加下面的代码:</p>
<pre>- pipeline.id: sync-posts-pipeline
  path.config: "/usr/share/logstash/pipeline/sync-posts.conf"
</pre>
<p>接下来，使用下面的代码在<code>logstash/pipeline</code>文件夹中添加一个<code>sync-posts.conf</code>文件，以设置输入和输出源:</p>
<pre>input {
    jdbc {
        jdbc_connection_string =&gt; "jdbc:postgresql://${POSTGRES_HOST}:5432/${POSTGRES_DB}"
        jdbc_user =&gt; "${POSTGRES_USER}"
        jdbc_password =&gt; "${POSTGRES_PASSWORD}"
        jdbc_driver_library =&gt; "/opt/logstash/vendor/jdbc/postgresql-42.2.18.jar"
        jdbc_driver_class =&gt; "org.postgresql.Driver"
        statement_filepath =&gt; "/usr/share/logstash/config/queries/sync-posts.sql"
        use_column_value =&gt; true
        tracking_column =&gt; "id"
        tracking_column_type =&gt; "numeric"
        schedule =&gt; "*/5 * * * * *"
    }
}

filter {
    mutate {
        remove_field =&gt; ["@version", "@timestamp"]
    }
}

output {
    if [operation] == "delete" {
        elasticsearch {
            hosts =&gt; ["http://elasticsearch:9200"] # URL of the ES docker container - docker would resolve it for us.
            action =&gt; "delete"
            index =&gt; "posts"
            document_id =&gt; "%{post_id}"
        }
    } else if [operation] in ["insert", "update"] {
        elasticsearch {
            hosts =&gt; ["http://elasticsearch:9200"]
            action =&gt; "index"
            index =&gt; "posts"
            document_id =&gt; "%{post_id}"
        }
    }
}
</pre>
<p>上面的配置文件由三个块组成:</p>
<ul>
<li><code>input</code> —使用<a href="https://www.elastic.co/guide/en/logstash/current/plugins-integrations-jdbc.html" target="_blank" rel="noopener"> JDBC </a>插件建立到 PostgreSQL 的连接，并指示 Logstash 每五秒运行一次由<code>statement_filepath</code>指定的 SQL 查询(由<code>schedule</code>值配置)。虽然<em> schedule </em>有类似 cron 的语法，但它也支持分钟间隔，并在幕后使用<a href="https://github.com/jmettraux/rufus-scheduler" target="_blank" rel="noopener"> rufus-scheduler </a>。你可以在这里了解更多关于语法和配置的信息<a href="https://github.com/jmettraux/rufus-scheduler#parsing-cronlines-and-time-strings" target="_blank" rel="noopener">。我们还跟踪<code>id</code>列，以便 Logstash 只获取自管道上次运行以来记录的操作</a></li>
<li><code>filter</code> —删除不需要的字段，包括由 Logstash 添加的字段</li>
<li><code>output</code> —负责将输入数据移入我们的弹性搜索索引。它使用 es 条件从索引中删除文档(如果数据库中的操作字段是 delete)或创建/更新文档(如果操作是 insert 或 update)</li>
</ul>
<p><em>你可以在<a href="https://www.elastic.co/guide/en/logstash/current/input-plugins.html" target="_blank" rel="noopener">输入</a>、<a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html" target="_blank" rel="noopener">过滤器</a>和<a href="https://www.elastic.co/guide/en/logstash/current/output-plugins.html" target="_blank" rel="noopener">输出</a>插件上探索日志存储文档，以查看每个块中可能的更多内容。</em></p>
<p>接下来，在<code>logstash/queries</code>中创建一个<code>sync-posts.sql</code>文件来存放我们管道的 SQL 语句:</p>
<pre>SELECT l.id,
       l.operation,
       l.post_id,
       p.id,
       p.title,
       p.body
FROM post_logs l
         LEFT JOIN posts p
                   ON p.id = l.post_id
WHERE l.id &gt; :sql_last_value ORDER BY l.id;
</pre>
<p><em> SELECT </em>语句使用 SQL 连接来获取基于<em> post_logs </em>表中的<code>post_id</code>的相关帖子。</p>
<p>配置好 Logstash 后，我们现在可以设置它的 Dockerfile 并将其添加到 docker-compose 服务中。在<code>logstash</code>文件夹中创建一个名为<code>Dockerfile</code>的新文件，并将下面的代码添加到其中:</p>
<pre>FROM docker.elastic.co/logstash/logstash:7.10.2

RUN /opt/logstash/bin/logstash-plugin install logstash-integration-jdbc
RUN mkdir /opt/logstash/vendor/jdbc
RUN curl -o /opt/logstash/vendor/jdbc/postgresql-42.2.18.jar https://jdbc.postgresql.org/download/postgresql-42.2.18.jar

ENTRYPOINT ["/usr/local/bin/docker-entrypoint"]
</pre>
<p>上面的 docker 文件获取了官方的 Logstash 映像，并设置了我们的管道需要的 JDBC 插件以及 PostgreSQL JDBC 驱动程序。</p>
<p>通过将 Logstash 添加到服务列表(即在<code>volumes</code>块之前)来更新<code>docker-compose.yml</code>文件，如下所示:</p>
<pre>logstash:
  build:
    context: logstash
  env_file: .env
  volumes:
    - ./logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml
    - ./logstash/pipelines/:/usr/share/logstash/pipeline/
    - ./logstash/queries/:/usr/share/logstash/config/queries/
  depends_on:
    - postgres
    - elasticsearch
</pre>
<p>Logstash 服务使用包含 Dockerfile 的<code>logstash</code>目录作为其上下文。它还使用卷将之前的配置文件装载到 Logstash 容器中的适当目录中。</p>
<h2>构建我们的 API 二进制</h2>
<p>我们现在准备将我们的项目公开为 HTTP API。我们将通过驻留在<code>cmd/api</code>中的<code>main.go</code>来实现这一点。在编辑器中打开它，并将下面的代码添加到其中:</p>
<pre>package main

import (
   "github.com/elastic/go-elasticsearch/v7"
   "os"
   "strconv"

   "github.com/gin-gonic/gin"
   "github.com/rs/zerolog"
   "gitlab.com/idoko/letterpress/db"
   "gitlab.com/idoko/letterpress/handler"
)

func main() {
   var dbPort int
   var err error
   logger := zerolog.New(os.Stderr).With().Timestamp().Logger()

   port := os.Getenv("POSTGRES_PORT")
   if dbPort, err = strconv.Atoi(port); err != nil {
      logger.Err(err).Msg("failed to parse database port")
      os.Exit(1)
   }
   dbConfig := db.Config{
      Host:     os.Getenv("POSTGRES_HOST"),
      Port:     dbPort,
      Username: os.Getenv("POSTGRES_USER"),
      Password: os.Getenv("POSTGRES_PASSWORD"),
      DbName:   os.Getenv("POSTGRES_DB"),
      Logger: logger,
   }
   logger.Info().Interface("config", &amp;dbConfig).Msg("config:")
   dbInstance, err := db.Init(dbConfig)
   if err != nil {
      logger.Err(err).Msg("Connection failed")
      os.Exit(1)
   }
   logger.Info().Msg("Database connection established")

   esClient, err := elasticsearch.NewDefaultClient()
   if err != nil {
      logger.Err(err).Msg("Connection failed")
      os.Exit(1)
   }

   h := handler.New(dbInstance, esClient, logger)
   router := gin.Default()
   rg := router.Group("/v1")
   h.Register(rg)
   router.Run(":8080")
}
</pre>
<p>首先，我们设置一个日志记录器，并将其传递给所有应用程序组件，以确保错误和事件日志是一致的。接下来，我们使用环境变量的值建立一个数据库连接(由<code>.env</code>文件管理)。我们还连接到 Elasticsearch 服务器，并确保它是可到达的。接下来，我们初始化路由处理程序，并在端口 8080 上启动 API 服务器。注意，我们还使用 gin 的路由组将我们所有的路由放在一个<code>v1</code>名称空间下，这样，我们也为我们的 API 提供了一种“版本控制”。</p>
<h2>测试我们的搜索应用</h2>
<p>现在，我们可以试用我们的搜索应用程序了。通过在终端中运行<code>docker-compose up --build</code>来重建并启动 docker-compose 服务。该命令还应该在<a href="http://localhost:8080" rel="nofollow"> http://localhost:8080 </a>上启动 API 服务器。</p>
<p>调出自己喜欢的 API 测试工具(例如<a href="https://www.postman.com/" target="_blank" rel="noopener"> Postman </a>、<a href="https://curl.se/" target="_blank" rel="noopener"> cURL </a>、<a href="https://httpie.io/" target="_blank" rel="noopener"> HTTPie </a>等)。)并创建一些帖子。在下面的例子中，我使用 HTTPie 向我们的数据库添加了五个不同的帖子(来自 Creative Commons 博客):</p>
<pre>$ http POST localhost:8080/v1/posts title="Meet CC South Africa, Our Next Feature for CC Network Fridays" body="After introducing the CC Italy Chapter to you in July, the CC Netherlands Chapter in August, CC Bangladesh Chapter in September, CC Tanzania Chapter in October, and the CC India Chapter in November, the CC Mexico Chapter in December, and CC Argentina Chapter in January, we are now traveling to Africa"

$ http POST localhost:8080/v1/posts title="Still Life: Art That Brings Comfort in Uncertain Times" body="There is a quiet, familiar beauty found in still life, a type of art that depicts primarily inanimate objects, like animals, food, or flowers. These comforting images offer a sense of certainty and simplicity in uncertain and complex times. This could explain why over six million Instagram users have fallen in love with still life"

$ http POST localhost:8080/v1/posts title="Why Universal Access to Information Matters" body="The coronavirus outbreak not only sparked a health pandemic; it triggered an infodemic of misleading and fabricated news. As the virus spread, trolls and conspiracy theorists began pushing misinformation, and their deplorable tactics continue to this day."
</pre>
<p>如果您更喜欢使用 Postman，这里有一个与上面类似的 Postman 请求的截图:</p>
<p><img data-attachment-id="37111" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/postman-request-create-new-post/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png" data-orig-size="730,496" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Postman request to create new post" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post-300x204.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png" decoding="async" class="aligncenter size-full wp-image-37111 jetpack-lazy-image" src="../Images/0c672f060242a01958c9614e4aae3ed7.png" alt="Postman Request to Create New Post" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post-300x204.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="37111" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/postman-request-create-new-post/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png" data-orig-size="730,496" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Postman request to create new post" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post-300x204.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-37111" src="../Images/0c672f060242a01958c9614e4aae3ed7.png" alt="Postman Request to Create New Post" srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post-300x204.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-create-new-post.png"/></noscript>
<p>您还可以检查 docker-compose 日志(如果您没有在后台运行 docker-compose ),以查看 Logstash 如何索引新帖子。</p>
<p>要测试搜索端点，向<a href="http://localhost:8080/v1/search" rel="nofollow">HTTP://localhost:8080/v1/search</a>发出一个 HTTP GET 请求，如下图所示:</p>
<p><img data-attachment-id="37112" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/postman-request-search-posts/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png" data-orig-size="730,518" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Postman request to search posts" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts-300x213.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png" decoding="async" class="aligncenter size-full wp-image-37112 jetpack-lazy-image" src="../Images/332ef6c7b058804185ba3df5afa93493.png" alt="Postman Request to Search Posts" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts-300x213.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="37112" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/postman-request-search-posts/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png" data-orig-size="730,518" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Postman request to search posts" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts-300x213.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-37112" src="../Images/332ef6c7b058804185ba3df5afa93493.png" alt="Postman Request to Search Posts" srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts-300x213.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/postman-request-search-posts.png"/></noscript>
<h2>使用 Kibana 可视化弹性搜索</h2>
<p>虽然我们总是可以使用 Elasticsearch API 来查看我们的 Elasticsearch 服务器上正在发生的事情，或者查看当前索引中的文档，但有时在定制的仪表板中可视化和浏览这些信息会很有帮助。Kibana 允许我们这样做。通过将下面的代码添加到<code>services</code>部分(即在<code>logstash</code>服务之后，但在<code>volumes</code>部分之前)，更新 docker-compose 文件以包含 Kibana 服务:</p>
<pre>kibana:
  image: 'docker.elastic.co/kibana/kibana:7.10.2'
  ports:
    - "5601:5601"
  hostname: kibana
  depends_on:
    - elasticsearch
</pre>
<p>我们让 Kibana 依赖于 Elasticsearch 服务，因为如果 Elasticsearch 没有启动和运行，它将毫无用处。我们还公开了默认的 Kibana 端口，这样我们就可以从我们的开发机器上访问仪表板。</p>
<p>通过运行<code>docker-compose up</code>启动 docker-compose 服务(如果它们正在运行，您必须先用<code>docker-compose down</code>停止它们)。访问<a href="http://localhost:5601" rel="nofollow"> http://localhost:5601 </a>访问 Kibana 仪表板。</p>
<p><img data-attachment-id="37113" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/kibana-dashboard/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png" data-orig-size="730,434" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Kibana dashboard" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard-300x178.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png" decoding="async" class="aligncenter size-full wp-image-37113 jetpack-lazy-image" src="../Images/a482b1126b9b95ea856d3689d20b3f8d.png" alt="Kibana Dashboard" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard-300x178.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="37113" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/kibana-dashboard/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png" data-orig-size="730,434" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Kibana dashboard" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard-300x178.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-37113" src="../Images/a482b1126b9b95ea856d3689d20b3f8d.png" alt="Kibana Dashboard" srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard-300x178.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dashboard.png"/></noscript>
<p>您还可以使用 Dev 工具查看 posts 索引中的所有文档，或者在应用程序中使用它们之前尝试不同的搜索查询。在下面的截图中，我们使用<code>match_all</code>列出了所有被索引的帖子:</p>
<p><img data-attachment-id="37114" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/kibana-dev-console/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png" data-orig-size="730,418" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Kibana dev console" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console-300x172.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png" decoding="async" class="aligncenter size-full wp-image-37114 jetpack-lazy-image" src="../Images/8ad3fccc4b8faa4fb4f5da3d36cfbe56.png" alt="Kibana Dev Console" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console-300x172.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="37114" data-permalink="https://blog.logrocket.com/using-elasticsearch-logstash-and-kibana-with-go-applications/kibana-dev-console/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png" data-orig-size="730,418" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Kibana dev console" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console-300x172.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png" decoding="async" loading="lazy" class="aligncenter size-full wp-image-37114" src="../Images/8ad3fccc4b8faa4fb4f5da3d36cfbe56.png" alt="Kibana Dev Console" srcset="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console-300x172.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/03/kibana-dev-console.png"/></noscript>
<h2>结论</h2>
<p>在本文中，我们探索了如何使用 ELK 堆栈将“搜索”添加到我们的 Go 应用程序中。完整的源代码可在<a href="https://gitlab.com/idoko/letterpress" target="_blank" rel="noopener"> GitLab </a>上获得。如果你遇到了问题，可以在那里随意制造问题。</p><div class="code-block code-block-1">
<div class="blog-plug base-cta"><h2>使用<a class="signup" href="https://lp.logrocket.com/blg/signup" target="_blank" rel="noopener noreferrer"> LogRocket </a>消除传统错误报告的干扰</h2>
<a href="https://lp.logrocket.com/blg/signup" class="signup" target="_blank" rel="noopener noreferrer"><img class="alignnone size-full wp-image-46 jetpack-lazy-image" src="../Images/d6f5a5dd739296c1dd7aab3d5e77eeb9.png" alt="LogRocket Dashboard Free Trial Banner" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2017/03/1d0cd-1s_rmyo6nbrasp-xtvbaxfg.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2017/03/1d0cd-1s_rmyo6nbrasp-xtvbaxfg.png"/><noscript><img data-lazy-fallback="1" class="alignnone size-full wp-image-46" src="../Images/d6f5a5dd739296c1dd7aab3d5e77eeb9.png" alt="LogRocket Dashboard Free Trial Banner" data-original-src="https://blog.logrocket.com/wp-content/uploads/2017/03/1d0cd-1s_rmyo6nbrasp-xtvbaxfg.png"/></noscript></a>
<p><a href="https://lp.logrocket.com/blg/signup" target="_blank" rel="noopener noreferrer"> LogRocket </a>是一个数字体验分析解决方案，它可以保护您免受数百个假阳性错误警报的影响，只针对几个真正重要的项目。LogRocket 会告诉您应用程序中实际影响用户的最具影响力的 bug 和 UX 问题。</p>
<p>然后，使用具有深层技术遥测的会话重放来确切地查看用户看到了什么以及是什么导致了问题，就像你在他们身后看一样。</p>
<p>LogRocket 自动聚合客户端错误、JS 异常、前端性能指标和用户交互。然后 LogRocket 使用机器学习来告诉你哪些问题正在影响大多数用户，并提供你需要修复它的上下文。</p>
<p>关注重要的 bug—<a class="signup" href="https://lp.logrocket.com/blg/signup-issue-free" target="_blank" rel="noopener noreferrer">今天就试试 LogRocket】。</a></p></div></div>

<p class="clearfix"/>
<p class="clearfix"/>
</article>

</div>    
</body>
</html>