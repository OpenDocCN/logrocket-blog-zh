<html>
<head>
<title>Scrape a website with Python, Scrapy, and MongoDB - LogRocket Blog</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>用 Python、Scrapy 和 MongoDB - LogRocket 博客创建一个网站</h1>
<blockquote>原文：<a href="https://blog.logrocket.com/scrape-website-python-scrapy-mongodb/#0001-01-01">https://blog.logrocket.com/scrape-website-python-scrapy-mongodb/#0001-01-01</a></blockquote><div><article class="article-post">
<h2>介绍</h2>
<p>数据已经成为一种新的商品，而且是一种昂贵的商品。随着人们在网上创建无限的内容，不同网站上的数据量增加了，许多初创公司提出了需要这些数据的想法。不幸的是，由于时间和资金的限制，他们不能总是自己制作</p>
<p>这个问题的一个流行解决方案是<a href="https://blog.logrocket.com/build-python-web-scraper-beautiful-soup/" target="_blank" rel="noopener">网络爬行和抓取</a>。随着机器学习应用对数据需求的增加，网络爬行变得非常流行。网络爬虫读取网站的来源(使用标记)，这使得很容易找到要提取的模式。</p>
<p>然而，爬行器是低效的，因为它们将抓取 HTML 标签中的所有内容，然后开发人员必须验证和清理数据。这就是 Scrapy 这样的工具的用武之地。Scrapy 是一个网络抓取器，而不是一个爬虫，因此对它将收集的数据类型更有辨别能力。</p>
<p>在接下来的章节中，您将了解 Scrapy，Python 最流行的抓取框架，以及如何使用它。</p>
<h2>Scrapy 简介</h2>
<p>Scrapy 是一个用 Python 编写的快速、高级 web 爬行框架。它是免费和开源的，用于大规模的网页抓取。</p>
<p>Scrapy 利用蜘蛛，它决定了一个网站(或一组网站)应该如何抓取你想要的信息。蜘蛛是定义如何抓取网站，以及如何从一组页面中提取结构化数据的类。</p>
<h2>入门指南</h2>
<p>就像任何其他 Python 项目一样，最好创建一个单独的虚拟环境，这样库就不会搞乱现有的基础环境。本文假设您已经安装了 Python 3.3 或更高版本。</p>
<h3>1.创建虚拟环境</h3>
<p>本文将使用<code>.venv</code>名称作为虚拟环境。您可以随意更改它，但是，请确保在整个项目中使用相同的名称。</p>
<pre class="language-shell hljs">mkdir web-scraper
cd web-scraper
python3 -m venv .venv
</pre>
<h3>2.激活虚拟环境</h3>
<p>对于 Windows，请使用以下内容:</p>
<pre class="language-shell hljs">.venv\Scripts\activate
</pre>
<p>对于 Linux 和 OSX:</p>
<pre class="shell-python hljs">source .venv/bin/activate
</pre>
<p>该命令将启用新的虚拟环境。它是新的，因此不包含任何内容，所以您必须安装所有需要的库。</p>
<h3>3.设置 Scrapy</h3>
<p>因为 Scrapy 是一个框架，它会自动安装其他需要的库:</p>
<pre class="language-shell hljs">pip install scrapy
</pre>
<p>要安装 Scrapy，请遵循<a href="https://docs.scrapy.org/en/latest/intro/install.html" target="_blank" rel="noopener">官方文档</a>。</p>
<h2>刮痧日志火箭专题文章</h2>
<p>要理解任何框架，最好的方法是边做边学。说了这么多，还是刮刮 LogRocket 精选文章和各自的评论吧。</p>
<h3>基本设置</h3>
<p>让我们从创建一个空白项目开始:</p>
<pre class="language-python hljs">scrapy startproject logrocket
</pre>
<p>接下来，用下面的代码创建你的第一个蜘蛛:</p>
<pre class="language-python hljs">cd logrocket
scrapy genspider feature_article blog.logrocket.com
</pre>
<p>让我们看看目录结构是什么样子的:</p>
<pre class="language-shell hljs">web-scraper
├── .venv
└── logrocket
    ├── logrocket
    │   ├── __init__.py
    │   ├── items.py
    │   ├── middlewares.py
    │   ├── pipelines.py
    │   ├── settings.py
    │   └── spiders
    │       ├── __init__.py
    │       └── feature_article.py
    └── scrapy.cfg
</pre>
<h2>编写第一只蜘蛛</h2>
<p>现在项目已经成功建立，让我们创建我们的第一个蜘蛛，它将从<a href="https://blog.logrocket.com" target="_blank" rel="noopener"> LogRocket 博客</a>中抓取所有的特色文章。</p>
<p>打开<code>spiders/feature_article.py</code>文件。</p>
<p>让我们一步一步来，首先从博客页面获取特色文章:</p>
<pre class="language-python hljs">import scrapy

class FeatureArticleSpider(scrapy.Spider):
    name = 'feature_article'
    allowed_domains = ['blog.logrocket.com']
    start_urls = ['http://blog.logrocket.com']

    def parse(self, response):
        feature_articles = response.css("section.featured-posts div.card")
        for article in feature_articles:
            article_dict = {
                "heading": article.css("h2.card-title a::text").extract_first().strip(),
                "url": article.css("h2.card-title a::attr(href)").extract_first(),
                "author": article.css("span.author-meta span.post-name a::text").extract_first(),
                "published_on": article.css("span.author-meta span.post-date::text").extract_first(),
                "read_time": article.css("span.readingtime::text").extract_first(),
            }
            yield article_dict
</pre>
<p>在上面的代码中可以看到，<code>scrapy.Spider</code>定义了一些属性和方法。它们是:</p>
<ul>
<li><code>name</code>，定义了蜘蛛，在项目中必须是唯一的</li>
<li><code>allowed_domains</code>，允许我们抓取的域名列表</li>
<li>我们开始抓取的网址列表</li>
<li><code>parse()</code>，被调用来处理请求的响应。它通常解析响应，提取数据，并以<code>dict</code>的形式输出数据</li>
</ul>
<h2>选择正确的 CSS 元素</h2>
<p>在刮取的过程中，知道唯一标识您希望刮取的元素的最佳方式是很重要的。</p>
<p>最好的方法是在浏览器中检查元素。你可以很容易地在开发者工具菜单中看到 HTML 结构。</p>
<p><img data-attachment-id="81008" data-permalink="https://blog.logrocket.com/scrape-website-python-scrapy-mongodb/developer-tools-logrocket/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png" data-orig-size="730,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="developer-tools-LogRocket" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket-300x173.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png" decoding="async" class="size-full wp-image-81008 aligncenter jetpack-lazy-image" src="../Images/4ed5898387bfbb52f76b6ccb79b4d95a.png" alt="developer tools menu on the LogRocket Blog" data-lazy-srcset="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket-300x173.png 300w" data-lazy-sizes="(max-width: 730px) 100vw, 730px" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png"/></p><noscript><img data-lazy-fallback="1" data-attachment-id="81008" data-permalink="https://blog.logrocket.com/scrape-website-python-scrapy-mongodb/developer-tools-logrocket/" data-orig-file="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png" data-orig-size="730,420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="developer-tools-LogRocket" data-image-description="" data-image-caption="" data-medium-file="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket-300x173.png" data-large-file="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png" decoding="async" loading="lazy" class="size-full wp-image-81008 aligncenter" src="../Images/4ed5898387bfbb52f76b6ccb79b4d95a.png" alt="developer tools menu on the LogRocket Blog" srcset="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png 730w, https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket-300x173.png 300w" sizes="(max-width: 730px) 100vw, 730px" data-original-src="https://blog.logrocket.com/wp-content/uploads/2021/12/developer-tools-LogRocket.png"/></noscript>
<h2>运行第一个蜘蛛</h2>
<p>使用以下命令运行上面的蜘蛛程序:</p>
<pre class="language-python hljs">scrapy crawl feature_article
</pre>
<p>应该所有的特色文章都是这样的:</p>
<pre class="language-python hljs">...
...
{'heading': 'Understanding React’s ', 'url': 'https://blog.logrocket.com/understanding-react-useeffect-cleanup-function/', 'author': 'Chimezie Innocent', 'published_on': 'Oct 27, 2021', 'read_time': '6 min read'}
2021-11-09 19:00:18 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://blog.logrocket.com/&gt;
...
...
</pre>
<h2>介绍项目</h2>
<p>抓取的主要目标是提取非结构化数据，并将其转换为有意义的结构化数据。Items 提供了一个类似 dict 的 API，带有一些很棒的附加特性。你可以在这里阅读更多关于<a href="https://docs.scrapy.org/en/latest/topics/items.html" target="_blank" rel="noopener">物品的信息。</a></p>
<p>让我们创建第一个项目，通过它的属性来指定文章。这里我们用<code>dataclass</code>来定义。</p>
<p>用以下内容编辑<code>items.py</code>:</p>
<pre class="language-python hljs">from dataclasses import dataclass

@dataclass
class LogrocketArticleItem:
    _id: str
    heading: str
    url: str
    author: str
    published_on: str
    read_time: str
</pre>
<p>然后，更新<code>spiders/feature_article.py</code>文件，如下所示:</p>
<pre class="language-python hljs">import scrapy
from ..items import LogrocketArticleItem

class FeatureArticleSpider(scrapy.Spider):
    name = 'feature_article'
    allowed_domains = ['blog.logrocket.com']
    start_urls = ['http://blog.logrocket.com']

    def parse(self, response):
        feature_articles = response.css("section.featured-posts div.card")
        for article in feature_articles:
            article_obj = LogrocketArticleItem(
                _id = article.css("::attr('id')").extract_first(),
                heading = article.css("h2.card-title a::text").extract_first(),
                url = article.css("h2.card-title a::attr(href)").extract_first(),
                author = article.css("span.author-meta span.post-name a::text").extract_first(),
                published_on = article.css("span.author-meta span.post-date::text").extract_first(),
                read_time = article.css("span.readingtime::text").extract_first(),
            )
            yield article_obj
</pre>
<h2>获取每个帖子的评论</h2>
<p>让我们更深入地去创造蜘蛛。为了获得每篇文章的评论，您需要请求每篇文章的 url，然后获取评论。</p>
<p>为此，让我们首先创建用于注释的条目(<code>item.py</code>):</p>
<pre class="language-python hljs">@dataclass
class LogrocketArticleCommentItem:
    _id: str
    author: str
    content: str
    published: str
</pre>
<p>现在注释项已经准备好了，让我们编辑<code>spiders/feature_article.py</code>，如下所示:</p>
<pre class="language-python hljs">import scrapy
from ..items import (
    LogrocketArticleItem,
    LogrocketArticleCommentItem
)

class FeatureArticleSpider(scrapy.Spider):
    name = 'feature_article'
    allowed_domains = ['blog.logrocket.com']
    start_urls = ['http://blog.logrocket.com']

    def get_comments(self, response):
        """
        The callback method gets the response from each article url.
        It fetches the article comment obj, creates a list of comments, and returns dict with the list of comments and article id.
        """
        article_comments = response.css("ol.comment-list li")
        comments = list()
        for comment in article_comments:
            comment_obj = LogrocketArticleCommentItem(
                _id = comment.css("::attr('id')").extract_first(),
                # special case: author can be inside `a` or `b` tag, so using xpath
                author = comment.xpath("string(//div[@class='comment-author vcard']//b)").get(),
                # special case: there can be multiple p tags, so for fetching all p tag inside content, xpath is used.
                content = comment.xpath("string(//div[@class='comment-content']//p)").get(),
                published = comment.css("div.comment-metadata a time::text").extract_first(),
            )
            comments.append(comment_obj)

        yield {"comments": comments, "article_id": response.meta.get("article_id")}

    def get_article_obj(self, article):
        """
        Creates an ArticleItem by populating the item values.
        """
        article_obj = LogrocketArticleItem(
            _id = article.css("::attr('id')").extract_first(),
            heading = article.css("h2.card-title a::text").extract_first(),
            url = article.css("h2.card-title a::attr(href)").extract_first(),
            author = article.css("span.author-meta span.post-name a::text").extract_first(),
            published_on = article.css("span.author-meta span.post-date::text").extract_first(),
            read_time = article.css("span.readingtime::text").extract_first(),
        )
        return article_obj

    def parse(self, response):
        """
        Main Method: loop through each article and yield the article.
        Also raises a request with the article url and yields the same.
        """
        feature_articles = response.css("section.featured-posts div.card")
        for article in feature_articles:
            article_obj = self.get_article_obj(article)
            # yield the article object
            yield article_obj
            # yield the comments for the article
            yield scrapy.Request(
                url = article_obj.url,
                callback = self.get_comments,
                meta={
                    "article_id": article_obj._id,
                }
            )
</pre>
<p>现在，用同样的命令运行上面的蜘蛛:</p>
<pre class="language-python hljs">scrapy crawl feature_article
</pre>
<h2>在 MongoDB 中持久化数据</h2>
<p>既然我们有了正确的数据，现在让我们将相同的数据持久化到数据库中。我们将使用 MongoDB 来存储刮下的项目。</p>
<h3>初始步骤</h3>
<p>在将 MongoDB 安装到您的系统中之后，使用 pip 安装<a href="https://pymongo.readthedocs.io/en/stable/" target="_blank" rel="noopener"> PyMongo </a>。PyMongo 是一个 Python 库，包含与 MongoDB 交互的工具。</p>
<pre class="language-shell hljs">pip3 install pymongo
</pre>
<p>接下来，在<code>settings.py</code>中添加新的 Mongo 相关设置。另外，在 Mongo 中创建一个数据库和集合:</p>
<pre class="language-python hljs"># MONGO DB SETTINGS
MONGO_HOST="localhost"
MONGO_PORT=27017
MONGO_DB_NAME="logrocket"
MONGO_COLLECTION_NAME="featured_articles"
</pre>
<h2>管道管理</h2>
<p>既然您已经设置了爬行和解析 HTML 的蜘蛛，并且设置了数据库设置。</p>
<p>接下来，我们必须通过<code>pipelines.py</code>中的管道将两者连接起来:</p>
<pre class="language-python hljs">from itemadapter import ItemAdapter
import pymongo
from scrapy.utils.project import get_project_settings
from .items import (
    LogrocketArticleCommentItem,
    LogrocketArticleItem
)
from dataclasses import asdict

settings = get_project_settings()

class MongoDBPipeline:
    def __init__(self):
        conn = pymongo.MongoClient(
            settings.get('MONGO_HOST'),
            settings.get('MONGO_PORT')
        )
        db = conn[settings.get('MONGO_DB_NAME')]
        self.collection = db[settings['MONGO_COLLECTION_NAME']]

    def process_item(self, item, spider):
        if isinstance(item, LogrocketArticleItem): # article item
            self.collection.update({"_id": item._id}, asdict(item), upsert = True)
        else:
            comments = []
            for comment in item.get("comments"):
                comments.append(asdict(comment))
            self.collection.update({"_id": item.get("article_id")}, {"$set": {"comments": comments} }, upsert=True)

        return item
</pre>
<p>在<code>settings.py</code>中添加该管道:</p>
<pre class="language-python hljs">USER_AGENT='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'
ITEM_PIPELINES = {'logrocket.pipelines.MongoDBPipeline': 100}
</pre>
<h2>最后试验</h2>
<p>再次运行爬网命令，并检查项目是否被正确推送到数据库:</p>
<pre class="language-python hljs">scrapy crawl feature_article
</pre>
<h2>结论</h2>
<p>在本指南中，您已经学习了如何用 Scrapy 编写基本的 spiders 并在数据库(MongoDB)中持久化抓取的数据。你只是触及了 Scrapy 作为一个网络抓取工具的表面，除了我们在这里讨论的，还有很多东西需要学习。</p>
<p>我希望从这篇文章中，你得到了 Scrapy 的基本知识，并有动力用这个奇妙的刮擦工具更深入地学习。</p><div class="code-block code-block-1">
<div class="blog-plug base-cta"><h2>使用<a class="signup" href="https://lp.logrocket.com/blg/signup" target="_blank" rel="noopener noreferrer"> LogRocket </a>消除传统错误报告的干扰</h2>
<a href="https://lp.logrocket.com/blg/signup" class="signup" target="_blank" rel="noopener noreferrer"><img class="alignnone size-full wp-image-46 jetpack-lazy-image" src="../Images/d6f5a5dd739296c1dd7aab3d5e77eeb9.png" alt="LogRocket Dashboard Free Trial Banner" data-lazy-src="https://blog.logrocket.com/wp-content/uploads/2017/03/1d0cd-1s_rmyo6nbrasp-xtvbaxfg.png?is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-original-src="https://blog.logrocket.com/wp-content/uploads/2017/03/1d0cd-1s_rmyo6nbrasp-xtvbaxfg.png"/><noscript><img data-lazy-fallback="1" class="alignnone size-full wp-image-46" src="../Images/d6f5a5dd739296c1dd7aab3d5e77eeb9.png" alt="LogRocket Dashboard Free Trial Banner" data-original-src="https://blog.logrocket.com/wp-content/uploads/2017/03/1d0cd-1s_rmyo6nbrasp-xtvbaxfg.png"/></noscript></a>
<p><a href="https://lp.logrocket.com/blg/signup" target="_blank" rel="noopener noreferrer"> LogRocket </a>是一个数字体验分析解决方案，它可以保护您免受数百个假阳性错误警报的影响，只针对几个真正重要的项目。LogRocket 会告诉您应用程序中实际影响用户的最具影响力的 bug 和 UX 问题。</p>
<p>然后，使用具有深层技术遥测的会话重放来确切地查看用户看到了什么以及是什么导致了问题，就像你在他们身后看一样。</p>
<p>LogRocket 自动聚合客户端错误、JS 异常、前端性能指标和用户交互。然后 LogRocket 使用机器学习来告诉你哪些问题正在影响大多数用户，并提供你需要修复它的上下文。</p>
<p>关注重要的 bug—<a class="signup" href="https://lp.logrocket.com/blg/signup-issue-free" target="_blank" rel="noopener noreferrer">今天就试试 LogRocket】。</a></p></div></div>

<p class="clearfix"/>
<p class="clearfix"/>
</article>

</div>    
</body>
</html>